{
  "modules": [
    {
      "id": "time-series-fundamentals",
      "title": "Time Series Fundamentals",
      "description": "Master the core concepts of time series analysis including stationarity, decomposition, and basic forecasting principles.",
      "duration": "60 minutes",
      "difficulty": "Beginner",
      "content": [
        {
          "type": "introduction",
          "title": "What is Time Series Analysis?",
          "content": "Time series analysis is a statistical technique that deals with time-ordered data. It's the backbone of forecasting in finance, economics, meteorology, and countless other fields where understanding temporal patterns is crucial.\n\n**Why Time Series Matters:**\n• Predict future stock prices and market trends\n• Forecast sales and demand for inventory management\n• Analyze climate patterns and weather prediction\n• Monitor and predict system performance metrics\n• Understand economic indicators and policy impacts"
        },
        {
          "type": "mathematical",
          "title": "Mathematical Foundation",
          "content": "**Formal Definition:**\nA time series {X_t} is a stochastic process indexed by time t, where:\n\nX_t = f(t) + ε_t\n\nWhere:\n• X_t = observed value at time t\n• f(t) = systematic component (trend + seasonality)\n• ε_t = random error term\n\n**Key Mathematical Properties:**\n• **Mean Function**: μ(t) = E[X_t]\n• **Variance Function**: σ²(t) = Var[X_t]\n• **Autocovariance**: γ(s,t) = Cov[X_s, X_t]\n• **Autocorrelation**: ρ(s,t) = γ(s,t)/√(γ(s,s)γ(t,t))"
        },
        {
          "type": "practical",
          "title": "Components Deep Dive",
          "content": "**1. TREND COMPONENT**\n• Linear Trend: X_t = α + βt + ε_t\n• Exponential Trend: X_t = αe^(βt) + ε_t\n• Polynomial Trend: X_t = α + βt + γt² + ε_t\n\n**Detection Methods:**\n• Visual inspection of plots\n• Mann-Kendall trend test\n• Linear regression significance\n\n**2. SEASONAL COMPONENT**\n• Additive: X_t = Trend_t + Seasonal_t + Error_t\n• Multiplicative: X_t = Trend_t × Seasonal_t × Error_t\n\n**Seasonal Patterns:**\n• Daily: 24-hour cycles (hourly data)\n• Weekly: 7-day cycles (daily data)\n• Monthly: 12-month cycles (monthly data)\n• Quarterly: 4-quarter cycles (quarterly data)\n\n**3. CYCLICAL COMPONENT**\n• Irregular fluctuations over 2+ years\n• Economic business cycles\n• No fixed period (unlike seasonality)\n• Often confused with long-term seasonality"
        },
        {
          "type": "code_example",
          "title": "Python Implementation Examples",
          "content": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Create sample time series\ndates = pd.date_range('2020-01-01', periods=100, freq='D')\nnp.random.seed(42)\ntrend = np.linspace(100, 200, 100)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(100) / 7)  # Weekly pattern\nnoise = np.random.normal(0, 5, 100)\nts = pd.Series(trend + seasonal + noise, index=dates)\n\n# Decomposition\ndecomposition = seasonal_decompose(ts, model='additive', period=7)\n\n# Plot components\nfig, axes = plt.subplots(4, 1, figsize=(12, 10))\nts.plot(ax=axes[0], title='Original Time Series')\ndecomposition.trend.plot(ax=axes[1], title='Trend')\ndecomposition.seasonal.plot(ax=axes[2], title='Seasonal')\ndecomposition.resid.plot(ax=axes[3], title='Residual')\nplt.tight_layout()\n```"
        },
        {
          "type": "case_study",
          "title": "Real-World Case Study: Retail Sales Analysis",
          "content": "**Problem**: A retail chain wants to understand their monthly sales patterns to optimize inventory and staffing.\n\n**Dataset**: 5 years of monthly sales data\n• Observations: 60 data points\n• Clear seasonal pattern (holiday shopping)\n• Upward trend due to business growth\n• Some irregular spikes (promotional events)\n\n**Analysis Steps:**\n1. **Visual Exploration**: Plot reveals clear Christmas/holiday peaks\n2. **Decomposition**: Separate trend (5% annual growth), seasonality (December +40%, February -20%)\n3. **Stationarity Testing**: Raw data non-stationary due to trend\n4. **Transformation**: Differencing removes trend, log transform stabilizes variance\n\n**Business Insights:**\n• December sales average 40% above baseline\n• February is consistently the weakest month\n• Year-over-year growth rate is stable at 5%\n• Promotional events add 10-15% temporary boost\n\n**Actionable Recommendations:**\n• Increase December inventory by 50%\n• Plan February promotions to boost sales\n• Staff seasonally based on predicted patterns\n• Budget for 5% annual growth in infrastructure"
        },
        {
          "type": "mathematical",
          "title": "Stationarity: The Foundation",
          "content": "**Weak Stationarity (Covariance Stationarity)**\nA time series {X_t} is weakly stationary if:\n\n1. **Constant Mean**: E[X_t] = μ for all t\n2. **Constant Variance**: Var[X_t] = σ² for all t\n3. **Time-Independent Covariance**: Cov[X_t, X_{t+k}] = γ(k)\n\n**Strong Stationarity**\nThe joint distribution of (X_{t1}, X_{t2}, ..., X_{tn}) is the same as (X_{t1+k}, X_{t2+k}, ..., X_{tn+k}) for any k and n.\n\n**Why Stationarity Matters:**\n• Most forecasting models assume stationarity\n• Allows use of historical data to predict future\n• Enables consistent parameter estimation\n• Statistical properties don't change over time\n\n**Testing for Stationarity:**\n• **Augmented Dickey-Fuller Test**: H0: Unit root exists (non-stationary)\n• **KPSS Test**: H0: Series is stationary\n• **Phillips-Perron Test**: Robust to heteroscedasticity\n• **Visual Methods**: ACF dies down slowly for non-stationary series"
        },
        {
          "type": "practical",
          "title": "Making Series Stationary",
          "content": "**Differencing:**\n• First Difference: ∇X_t = X_t - X_{t-1}\n• Seasonal Difference: ∇_s X_t = X_t - X_{t-s}\n• Second Difference: ∇²X_t = ∇X_t - ∇X_{t-1}\n\n**Transformations:**\n• Log Transform: Y_t = log(X_t) [for exponential growth]\n• Box-Cox Transform: Y_t = (X_t^λ - 1)/λ [for varying variance]\n• Square Root: Y_t = √X_t [for count data]\n\n**Detrending:**\n• Linear Detrending: Remove fitted linear trend\n• Moving Average Detrending: Subtract moving average\n• Hodrick-Prescott Filter: Statistical smoothing\n\n**When to Use Each Method:**\n• **Differencing**: Stochastic trends, random walks\n• **Log Transform**: Exponential growth, multiplicative effects\n• **Detrending**: Deterministic trends, business cycle analysis\n• **Seasonal Adjustment**: Remove known seasonal patterns"
        }
      ],
      "quiz": {
        "questions": [
          {
            "id": 1,
            "type": "multiple_choice",
            "question": "Which mathematical property is NOT required for weak stationarity?",
            "options": [
              "Constant mean over time",
              "Constant variance over time", 
              "Normal distribution of residuals",
              "Time-independent autocovariance"
            ],
            "correct_answer": 2,
            "explanation": "Weak stationarity requires constant mean, variance, and time-independent covariance. Normal distribution is not required for stationarity, though it's often assumed for inference."
          },
          {
            "id": 2,
            "type": "true_false",
            "question": "A time series with a clear upward trend can still be considered stationary.",
            "correct_answer": false,
            "explanation": "A series with a trend has a non-constant mean that changes over time, violating the stationarity requirement. The trend must be removed through differencing or detrending."
          },
          {
            "id": 3,
            "type": "coding",
            "question": "Write Python code to test if a pandas Series 'ts' is stationary using the Augmented Dickey-Fuller test (use statsmodels.tsa.stattools.adfuller). Print whether the series is stationary based on p-value < 0.05.",
            "correct_answer": "from statsmodels.tsa.stattools import adfuller\nresult = adfuller(ts)\nif result[1] < 0.05:\n    print('Series is stationary')\nelse:\n    print('Series is non-stationary')",
            "explanation": "The ADF test has null hypothesis of non-stationarity. If p-value < 0.05, we reject the null and conclude the series is stationary."
          },
          {
            "id": 4,
            "type": "multiple_choice",
            "question": "Which transformation is most appropriate for stabilizing variance in a time series with exponential growth?",
            "options": [
              "First differencing",
              "Log transformation",
              "Square root transformation",
              "Seasonal differencing"
            ],
            "correct_answer": 1,
            "explanation": "Log transformation is ideal for exponential growth patterns as it converts multiplicative relationships to additive ones and stabilizes variance proportional to the level."
          },
          {
            "id": 5,
            "type": "true_false",
            "question": "White noise is characterized by zero mean, constant variance, and no autocorrelation at any lag.",
            "correct_answer": true,
            "explanation": "White noise is a purely random process with E[εₜ] = 0, Var[εₜ] = σ², and Cov[εₜ, εₛ] = 0 for all t ≠ s, making it the ideal residual pattern after model fitting."
          },
          {
            "id": 6,
            "type": "multiple_choice",
            "question": "In seasonal decomposition, what does a multiplicative model assume about the relationship between components?",
            "options": [
              "Components are added together",
              "Seasonal amplitude is constant over time",
              "Seasonal amplitude varies proportionally with trend level",
              "There is no relationship between components"
            ],
            "correct_answer": 2,
            "explanation": "Multiplicative models (Y = T × S × E) assume seasonal fluctuations scale with the trend level, common in economic data where percentage changes are more meaningful than absolute changes."
          },
          {
            "id": 7,
            "type": "coding",
            "question": "Write Python code using pandas to create a time series with 365 daily observations starting from '2023-01-01' with random values.",
            "correct_answer": "import pandas as pd\nimport numpy as np\ndates = pd.date_range('2023-01-01', periods=365, freq='D')\nts = pd.Series(np.random.randn(365), index=dates)",
            "explanation": "pd.date_range creates a datetime index with specified frequency, and pd.Series combines it with random data to create a time series with proper temporal indexing."
          }
        ]
      }
    },
    {
      "id": "arima-models",
      "title": "ARIMA Models: The Foundation of Time Series Forecasting",
      "description": "Master AutoRegressive Integrated Moving Average models - the cornerstone of time series forecasting with deep mathematical understanding and practical implementation.",
      "duration": "90 minutes", 
      "difficulty": "Intermediate",
      "content": [
        {
          "type": "introduction",
          "title": "ARIMA: The Gold Standard",
          "content": "ARIMA (AutoRegressive Integrated Moving Average) models are among the most widely used and successful time series forecasting methods. Developed by Box and Jenkins in the 1970s, ARIMA models have stood the test of time and remain the benchmark against which other methods are compared.\n\n**Why ARIMA Models Excel:**\n• **Theoretical Foundation**: Based on solid mathematical principles\n• **Flexibility**: Can model a wide variety of time series patterns\n• **Interpretability**: Parameters have clear statistical meaning\n• **Proven Track Record**: Decades of successful applications\n• **Automation**: Model selection can be automated\n• **Forecast Intervals**: Provides uncertainty quantification"
        },
        {
          "type": "mathematical",
          "title": "Mathematical Framework",
          "content": "**ARIMA(p,d,q) Model Structure:**\n\n(1 - φ₁L - φ₂L² - ... - φₚLᵖ)(1-L)ᵈXₜ = (1 + θ₁L + θ₂L² + ... + θₚLᵖ)εₜ\n\nWhere:\n• L = lag operator (LXₜ = Xₜ₋₁)\n• φᵢ = autoregressive parameters\n• θⱼ = moving average parameters\n• εₜ = white noise error term\n• d = degree of differencing\n\n**Component Breakdown:**\n\n**AR(p) - AutoRegressive:**\nXₜ = φ₁Xₜ₋₁ + φ₂Xₜ₋₂ + ... + φₚXₜ₋ₚ + εₜ\n\n**I(d) - Integrated:**\nApply differencing d times to achieve stationarity\n∇ᵈXₜ = (1-L)ᵈXₜ\n\n**MA(q) - Moving Average:**\nXₜ = εₜ + θ₁εₜ₋₁ + θ₂εₜ₋₂ + ... + θₚεₜ₋ₚ\n\n**Stationarity Conditions:**\n• AR part: Roots of φ(z) = 0 must lie outside unit circle\n• MA part: Roots of θ(z) = 0 must lie outside unit circle for invertibility"
        },
        {
          "type": "practical", 
          "title": "Model Identification: The Box-Jenkins Methodology",
          "content": "**Step 1: Identification**\n\n**ACF (Autocorrelation Function) Patterns:**\n• **AR(p)**: ACF decays exponentially or as damped sinusoid\n• **MA(q)**: ACF cuts off after lag q\n• **ARMA(p,q)**: ACF decays exponentially after lag (q-p)\n\n**PACF (Partial Autocorrelation Function) Patterns:**\n• **AR(p)**: PACF cuts off after lag p\n• **MA(q)**: PACF decays exponentially or as damped sinusoid\n• **ARMA(p,q)**: PACF decays exponentially after lag (p-q)\n\n**Information Criteria:**\n• **AIC**: -2ln(L) + 2k (Akaike Information Criterion)\n• **BIC**: -2ln(L) + k*ln(n) (Bayesian Information Criterion)\n• **AICc**: AIC + 2k(k+1)/(n-k-1) (Corrected AIC for small samples)\n\n**Step 2: Estimation**\n• **Maximum Likelihood Estimation (MLE)**\n• **Least Squares Methods**\n• **Method of Moments**\n\n**Step 3: Diagnostic Checking**\n• **Ljung-Box Test**: H₀: Residuals are white noise\n• **Jarque-Bera Test**: H₀: Residuals are normally distributed\n• **ARCH Test**: H₀: No conditional heteroscedasticity\n• **Residual ACF/PACF**: Should show no significant correlation"
        },
        {
          "type": "code_example",
          "title": "Complete ARIMA Implementation",
          "content": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load and prepare data\ndata = pd.read_csv('airline_passengers.csv', parse_dates=['date'], index_col='date')\nts = data['passengers']\n\n# Step 1: Check stationarity\ndef check_stationarity(ts, title):\n    # ADF Test\n    adf_result = adfuller(ts)\n    print(f'{title} ADF Statistic: {adf_result[0]:.6f}')\n    print(f'P-value: {adf_result[1]:.6f}')\n    \n    if adf_result[1] <= 0.05:\n        print('Series is stationary')\n    else:\n        print('Series is non-stationary')\n\ncheck_stationarity(ts, 'Original Series')\n\n# Make stationary if needed\nts_diff = ts.diff().dropna()\ncheck_stationarity(ts_diff, 'First Differenced')\n\n# Step 2: Plot ACF and PACF\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Original series\nts.plot(ax=axes[0,0], title='Original Time Series')\nts_diff.plot(ax=axes[0,1], title='First Differenced Series')\n\n# ACF and PACF for differenced series\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplot_acf(ts_diff, ax=axes[1,0], lags=20, title='ACF')\nplot_pacf(ts_diff, ax=axes[1,1], lags=20, title='PACF')\nplt.tight_layout()\n\n# Step 3: Model selection using grid search\ndef evaluate_arima_model(data, arima_order):\n    try:\n        model = ARIMA(data, order=arima_order)\n        fitted_model = model.fit()\n        return fitted_model.aic\n    except:\n        return float('inf')\n\n# Grid search for best parameters\np_values = range(0, 4)\nd_values = range(0, 2)\nq_values = range(0, 4)\n\nbest_aic = float('inf')\nbest_order = None\n\nfor p in p_values:\n    for d in d_values:\n        for q in q_values:\n            aic = evaluate_arima_model(ts, (p, d, q))\n            if aic < best_aic:\n                best_aic = aic\n                best_order = (p, d, q)\n\nprint(f'Best ARIMA{best_order} with AIC: {best_aic:.2f}')\n\n# Step 4: Fit the best model\nbest_model = ARIMA(ts, order=best_order)\nfitted_model = best_model.fit()\n\n# Model summary\nprint(fitted_model.summary())\n\n# Step 5: Diagnostics\nresiduals = fitted_model.resid\n\n# Ljung-Box test for residual autocorrelation\nlb_test = acorr_ljungbox(residuals, lags=10, return_df=True)\nprint('\\nLjung-Box Test Results:')\nprint(lb_test)\n\n# Step 6: Forecasting\nforecast_steps = 12\nforecast = fitted_model.forecast(steps=forecast_steps)\nforecast_ci = fitted_model.get_forecast(steps=forecast_steps).conf_int()\n\n# Plot results\nfig, ax = plt.subplots(figsize=(12, 6))\nts.plot(ax=ax, label='Historical', color='blue')\nforecast.plot(ax=ax, label='Forecast', color='red')\nax.fill_between(forecast.index, \n                forecast_ci.iloc[:, 0], \n                forecast_ci.iloc[:, 1], \n                color='red', alpha=0.3)\nax.legend()\nax.set_title(f'ARIMA{best_order} Forecast')\nplt.show()\n\n# Step 7: Model evaluation metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# In-sample fit\nfitted_values = fitted_model.fittedvalues\nmae = mean_absolute_error(ts[1:], fitted_values)  # Skip first value due to differencing\nrmse = np.sqrt(mean_squared_error(ts[1:], fitted_values))\nmape = np.mean(np.abs((ts[1:] - fitted_values) / ts[1:])) * 100\n\nprint(f'\\nModel Performance Metrics:')\nprint(f'MAE: {mae:.2f}')\nprint(f'RMSE: {rmse:.2f}')\nprint(f'MAPE: {mape:.2f}%')\n```"
        },
        {
          "type": "case_study",
          "title": "Advanced Case Study: Economic Indicator Forecasting",
          "content": "**Problem**: Central bank needs to forecast quarterly GDP growth to inform monetary policy decisions.\n\n**Dataset**: 40 years of quarterly GDP data (160 observations)\n• Strong trend component (economic growth)\n• Cyclical patterns (business cycles)\n• External shocks (recessions, crises)\n• Seasonal effects (quarterly patterns)\n\n**Challenges:**\n• **Structural Breaks**: Economic crises change model dynamics\n• **Limited Data**: Only 160 observations for complex patterns\n• **High Stakes**: Policy decisions affect millions of people\n• **Real-time Forecasting**: Must work with preliminary data\n\n**Advanced ARIMA Techniques Applied:**\n\n**1. Seasonal ARIMA (SARIMA)**\n• Model: ARIMA(1,1,1)(1,1,1)₄\n• Accounts for quarterly seasonal patterns\n• Separate AR/MA terms for seasonal component\n\n**2. Intervention Analysis**\n• Dummy variables for known shocks (2008 crisis)\n• Step functions for permanent changes\n• Pulse functions for temporary effects\n\n**3. Model Validation Strategy**\n• **Out-of-sample Testing**: Reserve last 20 quarters\n• **Rolling Window**: Re-estimate every quarter\n• **Multiple Horizons**: Test 1, 2, 4, 8 quarter forecasts\n• **Encompassing Tests**: Compare against competing models\n\n**Results and Business Impact:**\n• **Forecast Accuracy**: MAPE of 1.2% for 1-quarter ahead\n• **Policy Insights**: Early warning of recession 2 quarters ahead\n• **Confidence Intervals**: 80% contain actual outcomes\n• **Model Stability**: Parameters stable over 10-year period\n\n**Lessons Learned:**\n• Simple ARIMA often outperforms complex alternatives\n• Regular model re-estimation crucial for structural change\n• Combining forecasts improves robustness\n• Expert judgment still valuable for extreme events"
        },
        {
          "type": "mathematical",
          "title": "Advanced Topics and Extensions",
          "content": "**Vector ARIMA (VARIMA) Models:**\nFor multivariate time series:\n\nΦ(L)(1-L)ᵈXₜ = Θ(L)εₜ\n\nWhere Xₜ is an n×1 vector and Φ(L), Θ(L) are matrix polynomials.\n\n**Fractional Integration ARFIMA(p,d,q):**\nAllows non-integer differencing parameter d:\n(1-L)ᵈ can be expanded using binomial theorem for |d| < 0.5\n\n**Threshold ARIMA (TAR):**\nDifferent ARIMA models for different regimes:\nXₜ = {\n  ARIMA₁ if Xₜ₋₁ ≤ threshold\n  ARIMA₂ if Xₜ₋₁ > threshold\n}\n\n**State Space Representation:**\nARIMA models can be written as:\n• State equation: αₜ₊₁ = Tαₜ + Rεₜ₊₁\n• Observation equation: Xₜ = Zαₜ\n\nEnables Kalman filtering for:\n• Missing value handling\n• Real-time updating\n• Structural time series modeling\n\n**Bayesian ARIMA:**\nPrior distributions on parameters:\n• φᵢ ~ N(0, σ²φ)\n• θⱼ ~ N(0, σ²θ)\n• σ²ε ~ InvGamma(a, b)\n\nAdvantages:\n• Uncertainty quantification\n• Natural regularization\n• Model averaging capabilities"
        }
      ],
      "quiz": {
        "questions": [
          {
            "id": 1,
            "type": "multiple_choice",
            "question": "In an ARIMA(2,1,1) model, what does the '1' in the middle position represent?",
            "options": [
              "One autoregressive parameter",
              "One degree of differencing",
              "One moving average parameter", 
              "One seasonal component"
            ],
            "correct_answer": 1,
            "explanation": "In ARIMA(p,d,q) notation, the middle parameter 'd' represents the degree of differencing needed to make the series stationary."
          },
          {
            "id": 2,
            "type": "multiple_choice",
            "question": "Which diagnostic test is primarily used to check for remaining autocorrelation in ARIMA residuals?",
            "options": [
              "Augmented Dickey-Fuller test",
              "Ljung-Box test",
              "Jarque-Bera test",
              "Breusch-Godfrey test"
            ],
            "correct_answer": 1,
            "explanation": "The Ljung-Box test is specifically designed to test the null hypothesis that residuals are independently distributed (white noise), which is crucial for ARIMA model validation."
          },
          {
            "id": 3,
            "type": "coding",
            "question": "Write Python code using statsmodels to fit an ARIMA(1,1,1) model to a pandas Series 'ts' and extract the AIC value.",
            "correct_answer": "from statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(ts, order=(1,1,1))\nfitted_model = model.fit()\naic_value = fitted_model.aic",
            "explanation": "This code creates an ARIMA model with the specified order, fits it to the data, and extracts the AIC value for model comparison."
          },
          {
            "id": 4,
            "type": "true_false", 
            "question": "A lower AIC value always indicates a better ARIMA model.",
            "correct_answer": true,
            "explanation": "AIC balances model fit and complexity. Lower AIC values indicate better models, as they achieve similar fit with fewer parameters or better fit with the same complexity."
          }
        ]
      }
    },
    {
      "id": "sarima-models",
      "title": "SARIMA Models: Mastering Seasonality",
      "description": "Advanced seasonal ARIMA models for complex time series with multiple seasonal patterns and sophisticated forecasting applications.",
      "duration": "75 minutes",
      "difficulty": "Intermediate",
      "content": [
        {
          "type": "introduction",
          "title": "The Power of Seasonal ARIMA",
          "content": "Seasonal ARIMA (SARIMA) models extend ARIMA to handle seasonal patterns that occur at regular intervals. Whether it's monthly sales peaks during holidays, weekly website traffic patterns, or daily temperature cycles, SARIMA provides the mathematical framework to model and forecast these complex seasonal behaviors.\n\n**Real-World Applications:**\n• **Retail**: Holiday shopping patterns, back-to-school sales\n• **Energy**: Daily/weekly electricity demand, seasonal heating/cooling\n• **Tourism**: Seasonal travel patterns, hotel occupancy rates\n• **Finance**: Quarterly earnings patterns, tax season effects\n• **Agriculture**: Crop yields, commodity prices with harvest cycles"
        },
        {
          "type": "mathematical",
          "title": "SARIMA Mathematical Framework",
          "content": "**SARIMA(p,d,q)(P,D,Q)ₛ Model:**\n\nΦ(L)Φₛ(Lˢ)(1-L)ᵈ(1-Lˢ)ᴰXₜ = Θ(L)Θₛ(Lˢ)εₜ\n\nWhere:\n• **Non-seasonal part**: φ(L) = 1 - φ₁L - ... - φₚLᵖ\n• **Seasonal AR**: Φₛ(Lˢ) = 1 - Φ₁Lˢ - ... - ΦₚL^(Ps)\n• **Seasonal MA**: Θₛ(Lˢ) = 1 + Θ₁Lˢ + ... + ΘᵩL^(Qs)\n• **s**: Seasonal period (12 for monthly, 4 for quarterly, 7 for daily)\n\n**Component Interactions:**\n• **Multiplicative Structure**: Seasonal and non-seasonal components interact\n• **Nested Seasonality**: Multiple seasonal patterns (daily + weekly + yearly)\n• **Seasonal Integration**: (1-Lˢ)ᴰ removes seasonal unit roots\n\n**Parameter Interpretation:**\n• **p,d,q**: Non-seasonal ARIMA parameters\n• **P,D,Q**: Seasonal ARIMA parameters at lag s\n• **D**: Usually 0 or 1 (seasonal differencing)\n• **P,Q**: Typically small values (1-3)"
        },
        {
          "type": "practical",
          "title": "Seasonal Pattern Identification",
          "content": "**Seasonal Diagnostics:**\n\n**1. Visual Methods:**\n• **Seasonal Plots**: Overlay same seasons across years\n• **Seasonal Subseries**: Box plots by season\n• **Lag Plots**: Scatter plots at seasonal lags\n• **Spectral Analysis**: Identify periodic components\n\n**2. Statistical Tests:**\n• **QS Test**: Seasonal unit root test\n• **HEGY Test**: Multiple seasonal unit roots\n• **Canova-Hansen Test**: Seasonal stability\n\n**3. ACF/PACF Patterns:**\n• **Seasonal Spikes**: Significant correlations at lags s, 2s, 3s\n• **Slow Decay**: ACF decreases slowly at seasonal lags\n• **Mixed Patterns**: Both seasonal and non-seasonal structure\n\n**Model Selection Strategy:**\n\n**Step 1**: Identify seasonal period (s)\n**Step 2**: Test for seasonal unit roots\n**Step 3**: Apply seasonal differencing if needed\n**Step 4**: Examine ACF/PACF at seasonal lags\n**Step 5**: Start with simple models: SARIMA(1,1,1)(1,1,1)ₛ\n**Step 6**: Use information criteria (AIC, BIC) for refinement"
        },
        {
          "type": "code_example",
          "title": "Complete SARIMA Implementation",
          "content": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load monthly airline passenger data\ndata = pd.read_csv('airline_passengers.csv', \n                   parse_dates=['Month'], index_col='Month')\nts = data['Passengers']\n\n# Step 1: Exploratory Data Analysis\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Original series\nts.plot(ax=axes[0,0], title='Monthly Airline Passengers')\n\n# Seasonal decomposition\ndecomposition = seasonal_decompose(ts, model='multiplicative', period=12)\ndecomposition.seasonal.plot(ax=axes[0,1], title='Seasonal Component')\ndecomposition.trend.plot(ax=axes[1,0], title='Trend Component')\ndecomposition.resid.plot(ax=axes[1,1], title='Residual Component')\nplt.tight_layout()\n\n# Step 2: Seasonal plots for pattern identification\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Seasonal plot\nfor year in range(1949, 1961):\n    yearly_data = ts[str(year)]\n    axes[0,0].plot(yearly_data.index.month, yearly_data.values, \n                   alpha=0.7, label=str(year))\naxes[0,0].set_title('Seasonal Plot - All Years Overlaid')\naxes[0,0].set_xlabel('Month')\naxes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Box plot by month\nts_monthly = ts.groupby(ts.index.month)\nmonthly_data = [ts_monthly.get_group(month).values for month in range(1, 13)]\naxes[0,1].boxplot(monthly_data, labels=range(1, 13))\naxes[0,1].set_title('Seasonal Subseries Plot')\naxes[0,1].set_xlabel('Month')\n\n# ACF and PACF\nplot_acf(ts, ax=axes[1,0], lags=50, title='ACF - Original Series')\nplot_pacf(ts, ax=axes[1,1], lags=50, title='PACF - Original Series')\nplt.tight_layout()\n\n# Step 3: Apply transformations\n# Log transformation to stabilize variance\nts_log = np.log(ts)\n\n# Seasonal differencing\nts_log_seasonal_diff = ts_log.diff(12).dropna()\n\n# Additional differencing if needed\nts_log_diff = ts_log_seasonal_diff.diff().dropna()\n\n# Check stationarity after transformations\nfrom statsmodels.tsa.stattools import adfuller\n\ndef check_stationarity(timeseries, title):\n    result = adfuller(timeseries, autolag='AIC')\n    print(f'\\n{title}')\n    print(f'ADF Statistic: {result[0]:.6f}')\n    print(f'p-value: {result[1]:.6f}')\n    print(f'Critical Values:')\n    for key, value in result[4].items():\n        print(f'\\t{key}: {value:.3f}')\n    \n    if result[1] <= 0.05:\n        print(\"Result: Reject null hypothesis - Series is stationary\")\n    else:\n        print(\"Result: Fail to reject null hypothesis - Series is non-stationary\")\n\ncheck_stationarity(ts, 'Original Series')\ncheck_stationarity(ts_log_seasonal_diff, 'Log + Seasonal Differencing')\ncheck_stationarity(ts_log_diff, 'Log + Seasonal + First Differencing')\n\n# Step 4: ACF/PACF analysis for transformed series\nfig, axes = plt.subplots(2, 2, figsize=(15, 8))\nplot_acf(ts_log_seasonal_diff, ax=axes[0,0], lags=40, title='ACF - Seasonally Differenced')\nplot_pacf(ts_log_seasonal_diff, ax=axes[0,1], lags=40, title='PACF - Seasonally Differenced')\nplot_acf(ts_log_diff, ax=axes[1,0], lags=40, title='ACF - Fully Differenced')\nplot_pacf(ts_log_diff, ax=axes[1,1], lags=40, title='PACF - Fully Differenced')\nplt.tight_layout()\n\n# Step 5: Automated model selection\ndef evaluate_sarima_model(data, order, seasonal_order, seasonal_period):\n    try:\n        model = SARIMAX(data, order=order, \n                        seasonal_order=seasonal_order + (seasonal_period,),\n                        enforce_stationarity=False, \n                        enforce_invertibility=False)\n        fitted_model = model.fit(disp=False)\n        return fitted_model.aic\n    except:\n        return float('inf')\n\n# Grid search for optimal parameters\np = d = q = range(0, 3)  # Non-seasonal parameters\nP = D = Q = range(0, 2)  # Seasonal parameters\ns = 12  # Monthly seasonality\n\nbest_aic = float('inf')\nbest_params = None\nbest_seasonal_params = None\n\nprint(\"Searching for optimal SARIMA parameters...\")\nfor param in itertools.product(p, d, q):\n    for seasonal_param in itertools.product(P, D, Q):\n        try:\n            aic = evaluate_sarima_model(ts_log, param, seasonal_param, s)\n            if aic < best_aic:\n                best_aic = aic\n                best_params = param\n                best_seasonal_params = seasonal_param\n                print(f'New best: SARIMA{param}x{seasonal_param + (s,)} - AIC: {aic:.2f}')\n        except:\n            continue\n\nprint(f'\\nBest model: SARIMA{best_params}x{best_seasonal_params + (s,)} with AIC: {best_aic:.2f}')\n\n# Step 6: Fit the best model\nbest_model = SARIMAX(ts_log, \n                     order=best_params,\n                     seasonal_order=best_seasonal_params + (s,),\n                     enforce_stationarity=False,\n                     enforce_invertibility=False)\n\nfitted_model = best_model.fit(disp=False)\nprint(fitted_model.summary())\n\n# Step 7: Model diagnostics\nresiduals = fitted_model.resid\nstandardized_residuals = residuals / np.std(residuals)\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Residual plots\nresiduals.plot(ax=axes[0,0], title='Residuals')\nstandardized_residuals.plot(ax=axes[0,1], title='Standardized Residuals')\n\n# Q-Q plot\nfrom scipy import stats\nstats.probplot(standardized_residuals, dist=\"norm\", plot=axes[1,0])\naxes[1,0].set_title('Q-Q Plot')\n\n# ACF of residuals\nplot_acf(residuals, ax=axes[1,1], lags=40, title='ACF of Residuals')\nplt.tight_layout()\n\n# Ljung-Box test\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nlb_test = acorr_ljungbox(residuals, lags=20, return_df=True)\nprint('\\nLjung-Box Test for Residual Autocorrelation:')\nprint(lb_test.head())\n\n# Step 8: Forecasting\nforecast_steps = 24  # 2 years ahead\nforecast = fitted_model.get_forecast(steps=forecast_steps)\nforecast_mean = forecast.predicted_mean\nforecast_ci = forecast.conf_int()\n\n# Transform back to original scale\nforecast_original = np.exp(forecast_mean)\nforecast_ci_original = np.exp(forecast_ci)\n\n# Plot forecast\nfig, ax = plt.subplots(figsize=(15, 8))\n\n# Historical data (last 3 years + forecast)\nhistorical_subset = ts['1958':]\nhistorical_subset.plot(ax=ax, label='Historical', color='blue')\n\n# Forecast\nforecast_original.plot(ax=ax, label='Forecast', color='red')\nax.fill_between(forecast_original.index,\n                forecast_ci_original.iloc[:, 0],\n                forecast_ci_original.iloc[:, 1],\n                color='red', alpha=0.3, label='95% Confidence Interval')\n\nax.set_title(f'SARIMA{best_params}x{best_seasonal_params + (s,)} Forecast')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n# Step 9: Model performance evaluation\n# In-sample fit\nfitted_values_log = fitted_model.fittedvalues\nfitted_values_original = np.exp(fitted_values_log)\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Calculate metrics on original scale\nactual = ts[fitted_values_original.index]\nmae = mean_absolute_error(actual, fitted_values_original)\nrmse = np.sqrt(mean_squared_error(actual, fitted_values_original))\nmape = np.mean(np.abs((actual - fitted_values_original) / actual)) * 100\n\nprint(f'\\n=== Model Performance Metrics ===')\nprint(f'MAE: {mae:.2f}')\nprint(f'RMSE: {rmse:.2f}')\nprint(f'MAPE: {mape:.2f}%')\nprint(f'R-squared: {1 - np.var(actual - fitted_values_original) / np.var(actual):.4f}')\n\n# Forecast summary\nprint(f'\\n=== 24-Month Forecast Summary ===')\nprint(f'Average forecasted growth rate: {(forecast_original.iloc[-1]/ts.iloc[-1])**(1/2) - 1:.2%} annually')\nprint(f'Peak season (Dec 1962): {forecast_original.iloc[-2]:.0f} passengers')\nprint(f'Low season (Feb 1963): {forecast_original.iloc[1]:.0f} passengers')\n```"
        },
        {
          "type": "case_study",
          "title": "Industry Case Study: E-commerce Demand Forecasting",
          "content": "**Challenge**: Major online retailer needs accurate demand forecasts for inventory planning across 10,000 products with complex seasonal patterns.\n\n**Dataset Complexity:**\n• **Multiple Seasonalities**: Daily (day-of-week), weekly (paydays), monthly (month-end), yearly (holidays)\n• **External Events**: Black Friday, Cyber Monday, Back-to-school, Valentine's Day\n• **Product Lifecycles**: Launch effects, discontinuation, seasonal items\n• **Scale**: 3 years of daily data per product (1,095 observations × 10,000 products)\n\n**SARIMA Implementation Strategy:**\n\n**1. Hierarchical Approach:**\n• **Product Categories**: Electronics, Clothing, Home & Garden, Sports\n• **Seasonal Patterns**: Category-specific (e.g., winter coats vs. swimwear)\n• **Base Models**: SARIMA(1,1,1)(1,1,1)₇ for weekly patterns\n• **Nested Seasonality**: Additional yearly component for major categories\n\n**2. Automated Pipeline:**\n```python\nclass ProductForecaster:\n    def __init__(self, category):\n        self.category = category\n        self.seasonal_periods = self._get_seasonal_periods()\n    \n    def _get_seasonal_periods(self):\n        # Category-specific seasonal patterns\n        patterns = {\n            'electronics': [7, 365],  # Weekly + yearly\n            'clothing': [7, 91, 365], # Weekly + quarterly + yearly\n            'home_garden': [7, 365],   # Weekly + yearly\n            'sports': [7, 365]        # Weekly + yearly\n        }\n        return patterns.get(self.category, [7])\n    \n    def fit_sarima(self, data):\n        # Automated model selection with category constraints\n        best_aic = float('inf')\n        for p, d, q in itertools.product([0,1,2], [0,1], [0,1,2]):\n            for P, D, Q in itertools.product([0,1], [0,1], [0,1]):\n                try:\n                    model = SARIMAX(data, order=(p,d,q), \n                                   seasonal_order=(P,D,Q,7))\n                    fitted = model.fit(disp=False)\n                    if fitted.aic < best_aic:\n                        best_aic = fitted.aic\n                        self.best_model = fitted\n                except:\n                    continue\n        return self.best_model\n```\n\n**3. Business Results:**\n• **Forecast Accuracy**: 15% improvement in MAPE over naive seasonal methods\n• **Inventory Optimization**: $2.3M reduction in excess inventory\n• **Stockout Reduction**: 23% fewer out-of-stock incidents during peak seasons\n• **Processing Time**: Automated pipeline processes all products in 4 hours\n\n**4. Key Insights:**\n• Simple SARIMA(1,1,1)(1,1,1)₇ worked best for 70% of products\n• Electronics showed strongest weekly patterns (weekend peaks)\n• Clothing required quarterly seasonality for fashion cycles\n• Holiday effects better modeled as intervention variables\n• Model performance degraded for products with <1 year of history\n\n**5. Operational Implementation:**\n• **Daily Updates**: Models re-fitted weekly with new data\n• **Exception Handling**: Manual review for products with poor model fit\n• **Integration**: Forecasts fed directly into ERP system\n• **Monitoring**: Automated alerts for forecast accuracy degradation"
        },
        {
          "type": "mathematical",
          "title": "Advanced SARIMA Extensions",
          "content": "**Multiplicative vs. Additive Seasonality:**\n\n**Additive Model:**\nX_t = Trend_t + Seasonal_t + Error_t\n• Seasonal amplitude constant over time\n• Use when seasonal fluctuations are roughly constant\n\n**Multiplicative Model:**\nX_t = Trend_t × Seasonal_t × Error_t\n• Seasonal amplitude proportional to trend level\n• More common in economic/business data\n• Requires log transformation: log(X_t) = log(Trend_t) + log(Seasonal_t) + log(Error_t)\n\n**Multiple Seasonal Periods:**\nFor data with nested seasonality (e.g., hourly data with daily + weekly patterns):\n\nSARIMA(p,d,q)(P₁,D₁,Q₁)ₛ₁(P₂,D₂,Q₂)ₓ₂\n\n**Vector SARIMA (VSARIMA):**\nFor multivariate seasonal time series:\nΦ(L)Φₛ(Lˢ)(1-L)ᵈ(1-Lˢ)ᴰX_t = Θ(L)Θₛ(Lˢ)ε_t\n\nWhere X_t is a vector of related time series.\n\n**Regime-Switching SARIMA:**\nDifferent SARIMA parameters for different economic states:\n• Expansion vs. recession regimes\n• High vs. low volatility periods\n• Normal vs. crisis conditions\n\n**Bayesian SARIMA:**\nPrior distributions on seasonal parameters:\n• Φₛ ~ N(0, τ²I)\n• Seasonal coefficients tend toward zero (shrinkage)\n• Automatic model selection through variable selection\n\n**Fractional Seasonal Integration:**\nSARFIMA models allow non-integer seasonal differencing:\n(1-Lˢ)ᴰˢ where Dₛ can be fractional\n\nUseful for:\n• Long-memory seasonal processes\n• Slowly decaying seasonal autocorrelations\n• Environmental data with persistent seasonal effects"
        }
      ],
      "quiz": {
        "questions": [
          {
            "id": 1,
            "type": "multiple_choice",
            "question": "In SARIMA(1,1,1)(1,1,1)₁₂ notation, what does the subscript 12 represent?",
            "options": [
              "12 autoregressive parameters",
              "12-month seasonal period",
              "12 degrees of freedom",
              "12 forecast periods"
            ],
            "correct_answer": 1,
            "explanation": "The subscript s in SARIMA notation represents the seasonal period - in this case, 12 indicates monthly data with yearly seasonality."
          },
          {
            "id": 2,
            "type": "multiple_choice",
            "question": "When should you apply seasonal differencing (1-L^s) to a time series?",
            "options": [
              "When the series has a linear trend",
              "When there are seasonal unit roots",
              "When the variance is non-constant",
              "When residuals are correlated"
            ],
            "correct_answer": 1,
            "explanation": "Seasonal differencing (1-L^s) is applied when seasonal unit roots are present, meaning the seasonal pattern is non-stationary and persistent."
          },
          {
            "id": 3,
            "type": "true_false",
            "question": "SARIMA models can handle multiple seasonal periods simultaneously (e.g., both daily and weekly patterns).",
            "correct_answer": false,
            "explanation": "Standard SARIMA handles only one seasonal period. Multiple seasonal periods require extended models like multiple SARIMA or specialized methods."
          },
          {
            "id": 4,
            "type": "coding",
            "question": "Write Python code using statsmodels to fit a SARIMA(1,1,1)(1,1,1)₁₂ model to a pandas Series 'ts' and get the fitted model's AIC.",
            "correct_answer": "from statsmodels.tsa.statespace.sarimax import SARIMAX\nmodel = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12))\nfitted_model = model.fit()\naic = fitted_model.aic",
            "explanation": "SARIMAX is the modern statsmodels implementation for SARIMA. The seasonal_order tuple includes (P,D,Q,s) where s is the seasonal period."
          },
          {
            "id": 5,
            "type": "multiple_choice",
            "question": "What is the primary purpose of the Augmented Dickey-Fuller (ADF) test when building SARIMA models?",
            "options": [
              "To determine the seasonal period",
              "To test for stationarity and decide on differencing order",
              "To select the optimal AR and MA orders",
              "To validate model residuals"
            ],
            "correct_answer": 1,
            "explanation": "The ADF test checks for unit roots to determine if differencing is needed. A significant p-value (< 0.05) indicates stationarity, helping decide the differencing orders (d and D)."
          },
          {
            "id": 6,
            "type": "true_false",
            "question": "In SARIMA modeling, applying log transformation before differencing helps stabilize variance when seasonal fluctuations increase with the level.",
            "correct_answer": true,
            "explanation": "Log transformation is crucial for multiplicative seasonality where seasonal amplitude grows with the trend. This stabilizes variance before applying SARIMA, which assumes constant variance."
          },
          {
            "id": 7,
            "type": "multiple_choice",
            "question": "When ACF shows spikes at lags 12, 24, 36 and slowly decaying pattern, what does this suggest for monthly data?",
            "options": [
              "Use seasonal differencing (D=1) with s=12",
              "Increase the number of seasonal AR terms (P)",
              "Apply first differencing (d=1) only",
              "The data is already stationary"
            ],
            "correct_answer": 0,
            "explanation": "Slowly decaying spikes at seasonal lags (12, 24, 36) indicate a seasonal unit root, requiring seasonal differencing (D=1) with seasonal period s=12 for monthly data."
          }
        ]
      }
    },
    {
      "id": "holt-winters",
      "title": "Holt-Winters: Exponential Smoothing Excellence",
      "description": "Master exponential smoothing methods including Holt-Winters triple exponential smoothing for trend and seasonal forecasting with practical business applications.",
      "duration": "70 minutes",
      "difficulty": "Intermediate",
      "content": [
        {
          "type": "introduction",
          "title": "Evolution of Exponential Smoothing",
          "content": "Exponential smoothing methods provide an elegant alternative to ARIMA models, particularly effective for business forecasting where interpretability and computational efficiency are crucial. From simple exponential smoothing to the sophisticated Holt-Winters method, these techniques have powered demand forecasting systems for decades.\n\n**Method Hierarchy:**\n• **Simple Exponential Smoothing**: Level only (no trend/seasonality)\n• **Holt's Method**: Level + linear trend\n• **Holt-Winters**: Level + trend + seasonality\n• **Damped Trend Methods**: Non-linear trend dampening\n• **State Space Models**: Modern statistical framework (ETS)\n\n**Business Applications:**\n• **Retail**: Sales forecasting, inventory management\n• **Manufacturing**: Production planning, capacity utilization\n• **Finance**: Revenue projections, budget planning\n• **Supply Chain**: Demand sensing, supplier planning\n• **Energy**: Load forecasting, consumption patterns"
        },
        {
          "type": "mathematical",
          "title": "Holt-Winters Mathematical Framework",
          "content": "**Triple Exponential Smoothing (Holt-Winters):**\n\n**Additive Seasonality:**\n• **Level**: ℓₜ = α(xₜ - sₜ₋ₘ) + (1-α)(ℓₜ₋₁ + bₜ₋₁)\n• **Trend**: bₜ = β(ℓₜ - ℓₜ₋₁) + (1-β)bₜ₋₁\n• **Seasonality**: sₜ = γ(xₜ - ℓₜ₋₁ - bₜ₋₁) + (1-γ)sₜ₋ₘ\n• **Forecast**: x̂ₜ₊ₕ = ℓₜ + hbₜ + sₜ₊ₕ₋ₘ₍ₕ₎\n\n**Multiplicative Seasonality:**\n• **Level**: ℓₜ = α(xₜ/sₜ₋ₘ) + (1-α)(ℓₜ₋₁ + bₜ₋₁)\n• **Trend**: bₜ = β(ℓₜ - ℓₜ₋₁) + (1-β)bₜ₋₁\n• **Seasonality**: sₜ = γ(xₜ/(ℓₜ₋₁ + bₜ₋₁)) + (1-γ)sₜ₋ₘ\n• **Forecast**: x̂ₜ₊ₕ = (ℓₜ + hbₜ) × sₜ₊ₕ₋ₘ₍ₕ₎\n\n**Parameter Interpretation:**\n• **α (0≤α≤1)**: Level smoothing - higher values = more responsive to recent changes\n• **β (0≤β≤1)**: Trend smoothing - controls trend adaptation rate\n• **γ (0≤γ≤1)**: Seasonal smoothing - seasonal pattern update rate\n• **m**: Seasonal period length\n\n**State Space Representation (ETS Framework):**\n**Observation equation**: yₜ = ℓₜ₋₁ + bₜ₋₁ + sₜ₋ₘ + εₜ\n**State equations**:\n• ℓₜ = ℓₜ₋₁ + bₜ₋₁ + αεₜ\n• bₜ = bₜ₋₁ + βεₜ\n• sₜ = sₜ₋ₘ + γεₜ\n\n**Damped Trend Extension:**\n• **Trend equation**: bₜ = βφ(ℓₜ - ℓₜ₋₁) + (1-β)φbₜ₋₁\n• **Forecast**: x̂ₜ₊ₕ = ℓₜ + φₕbₜ + sₜ₊ₕ₋ₘ₍ₕ₎\n• **φ (0<φ≤1)**: Damping parameter - trends decay over time when φ<1"
        },
        {
          "type": "practical",
          "title": "Model Selection and Parameter Optimization",
          "content": "**ETS Model Selection Framework:**\n\n**Error Type**: (A)dditive or (M)ultiplicative\n**Trend Type**: (N)one, (A)dditive, or (A)damped\n**Seasonal Type**: (N)one, (A)dditive, or (M)ultiplicative\n\n**Common ETS Models:**\n• **ETS(A,N,N)**: Simple exponential smoothing\n• **ETS(A,A,N)**: Holt's linear trend\n• **ETS(A,A,A)**: Additive Holt-Winters\n• **ETS(A,A,M)**: Multiplicative Holt-Winters\n• **ETS(M,A,M)**: Multiplicative errors and seasonality\n• **ETS(A,Ad,A)**: Damped trend with additive seasonality\n\n**Automatic Model Selection:**\n\n**1. Information Criteria:**\n• **AIC**: Akaike Information Criterion\n• **BIC**: Bayesian Information Criterion\n• **AICc**: Corrected AIC for small samples\n\n**2. Cross-Validation:**\n• **Time Series CV**: Rolling origin validation\n• **Seasonal CV**: Leave-one-season-out\n• **Block CV**: Leave-one-year-out\n\n**3. Forecast Accuracy Metrics:**\n• **MASE**: Mean Absolute Scaled Error\n• **sMAPE**: Symmetric Mean Absolute Percentage Error\n• **RMSSE**: Root Mean Squared Scaled Error\n\n**Parameter Optimization:**\n\n**1. Grid Search**: Systematic parameter space exploration\n```\nα_values = [0.1, 0.3, 0.5, 0.7, 0.9]\nβ_values = [0.1, 0.3, 0.5, 0.7, 0.9]\nγ_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n```\n\n**2. Gradient-Based Optimization**: Maximum likelihood estimation\n• **L-BFGS-B**: Bounded optimization\n• **Nelder-Mead**: Simplex method\n• **SLSQP**: Sequential least squares\n\n**3. Bayesian Optimization**: For expensive evaluations\n• **Gaussian Process**: Model parameter response surface\n• **Acquisition Functions**: Balance exploration vs exploitation\n• **Sequential Design**: Adaptive parameter sampling\n\n**Model Diagnostics:**\n\n**1. Residual Analysis:**\n• **Independence**: Ljung-Box test\n• **Normality**: Shapiro-Wilk test\n• **Homoscedasticity**: Breusch-Pagan test\n\n**2. Component Decomposition:**\n• **Level evolution**: Smooth without jumps\n• **Trend stability**: Reasonable growth rates\n• **Seasonal consistency**: Stable seasonal indices\n\n**3. Forecast Evaluation:**\n• **Coverage**: Prediction intervals contain actual values\n• **Calibration**: Forecast errors match assumed distribution\n• **Directional accuracy**: Forecast direction matches actual"
        },
        {
          "type": "code_example",
          "title": "Complete Holt-Winters Implementation",
          "content": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport itertools\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load retail sales data\ndata = pd.read_csv('retail_sales.csv', parse_dates=['Date'], index_col='Date')\nts = data['Sales'].resample('M').sum()  # Monthly aggregation\n\nclass HoltWintersAnalyzer:\n    def __init__(self, data):\n        self.data = data\n        self.models = {}\n        self.forecasts = {}\n        self.metrics = {}\n    \n    def explore_seasonality(self):\n        \"\"\"Comprehensive seasonality analysis\"\"\"\n        fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n        \n        # Original time series\n        self.data.plot(ax=axes[0,0], title='Original Time Series')\n        axes[0,0].grid(True)\n        \n        # Seasonal decomposition\n        decomp_add = seasonal_decompose(self.data, model='additive', period=12)\n        decomp_mult = seasonal_decompose(self.data, model='multiplicative', period=12)\n        \n        decomp_add.seasonal.plot(ax=axes[0,1], title='Additive Seasonal Component', color='blue')\n        decomp_mult.seasonal.plot(ax=axes[1,0], title='Multiplicative Seasonal Component', color='red')\n        \n        # Seasonal subseries plot\n        monthly_data = []\n        for month in range(1, 13):\n            month_values = self.data[self.data.index.month == month]\n            monthly_data.append(month_values.values)\n        \n        axes[1,1].boxplot(monthly_data, labels=range(1, 13))\n        axes[1,1].set_title('Seasonal Subseries Plot')\n        axes[1,1].set_xlabel('Month')\n        \n        # Seasonal strength calculation\n        seasonal_strength_add = 1 - np.var(decomp_add.resid.dropna()) / np.var(decomp_add.seasonal + decomp_add.resid).dropna()\n        seasonal_strength_mult = 1 - np.var(decomp_mult.resid.dropna()) / np.var(decomp_mult.seasonal * decomp_mult.resid).dropna()\n        \n        # Plot seasonal indices\n        seasonal_indices_add = decomp_add.seasonal.groupby(decomp_add.seasonal.index.month).mean()\n        seasonal_indices_mult = decomp_mult.seasonal.groupby(decomp_mult.seasonal.index.month).mean()\n        \n        seasonal_indices_add.plot(ax=axes[2,0], kind='bar', title=f'Additive Seasonal Indices\\nStrength: {seasonal_strength_add:.3f}')\n        seasonal_indices_mult.plot(ax=axes[2,1], kind='bar', title=f'Multiplicative Seasonal Indices\\nStrength: {seasonal_strength_mult:.3f}')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        return seasonal_strength_add, seasonal_strength_mult\n    \n    def fit_all_models(self):\n        \"\"\"Fit all ETS model combinations\"\"\"\n        \n        # Define model space\n        trends = [None, 'add', 'add_damped']\n        seasonals = [None, 'add', 'mul']\n        \n        model_names = {\n            (None, None): 'Simple Exponential Smoothing',\n            ('add', None): \"Holt's Linear Trend\",\n            (None, 'add'): 'Seasonal Additive',\n            (None, 'mul'): 'Seasonal Multiplicative',\n            ('add', 'add'): 'Holt-Winters Additive',\n            ('add', 'mul'): 'Holt-Winters Multiplicative',\n            ('add_damped', 'add'): 'Damped Holt-Winters Additive',\n            ('add_damped', 'mul'): 'Damped Holt-Winters Multiplicative'\n        }\n        \n        results = []\n        \n        for trend in trends:\n            for seasonal in seasonals:\n                # Skip invalid combinations\n                if seasonal is not None and len(self.data) < 24:  # Need 2+ seasonal cycles\n                    continue\n                \n                try:\n                    # Fit model\n                    model = ExponentialSmoothing(\n                        self.data,\n                        trend=trend,\n                        seasonal=seasonal,\n                        seasonal_periods=12 if seasonal else None,\n                        damped_trend=(trend == 'add_damped')\n                    )\n                    \n                    fitted_model = model.fit(optimized=True, use_brute=True)\n                    \n                    # Store model\n                    model_key = (trend, seasonal)\n                    self.models[model_key] = fitted_model\n                    \n                    # Calculate metrics\n                    aic = fitted_model.aic\n                    bic = fitted_model.bic\n                    \n                    # In-sample fit\n                    fitted_values = fitted_model.fittedvalues\n                    residuals = self.data - fitted_values\n                    mae = mean_absolute_error(self.data[fitted_values.index], fitted_values)\n                    rmse = np.sqrt(mean_squared_error(self.data[fitted_values.index], fitted_values))\n                    \n                    results.append({\n                        'Model': model_names.get(model_key, f'Trend: {trend}, Seasonal: {seasonal}'),\n                        'Trend': trend,\n                        'Seasonal': seasonal,\n                        'AIC': aic,\n                        'BIC': bic,\n                        'MAE': mae,\n                        'RMSE': rmse,\n                        'Parameters': fitted_model.params\n                    })\n                    \n                    print(f\"✓ Fitted: {model_names.get(model_key)}\")\n                    \n                except Exception as e:\n                    print(f\"✗ Failed: {model_names.get((trend, seasonal), f'Trend: {trend}, Seasonal: {seasonal}')} - {str(e)}\")\n        \n        # Create results DataFrame\n        results_df = pd.DataFrame(results)\n        results_df = results_df.sort_values('AIC').reset_index(drop=True)\n        \n        print(\"\\n=== Model Comparison Results ===\")\n        print(results_df[['Model', 'AIC', 'BIC', 'MAE', 'RMSE']].round(3))\n        \n        # Best model\n        best_model_info = results_df.iloc[0]\n        best_model_key = (best_model_info['Trend'], best_model_info['Seasonal'])\n        self.best_model = self.models[best_model_key]\n        \n        print(f\"\\nBest Model: {best_model_info['Model']}\")\n        print(f\"Parameters: {best_model_info['Parameters']}\")\n        \n        return results_df\n    \n    def analyze_components(self):\n        \"\"\"Analyze model components and parameters\"\"\"\n        if not hasattr(self, 'best_model'):\n            raise ValueError(\"Must fit models first using fit_all_models()\")\n        \n        fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n        \n        # Original vs Fitted\n        self.data.plot(ax=axes[0,0], label='Actual', color='blue')\n        self.best_model.fittedvalues.plot(ax=axes[0,0], label='Fitted', color='red', alpha=0.7)\n        axes[0,0].legend()\n        axes[0,0].set_title('Actual vs Fitted Values')\n        axes[0,0].grid(True)\n        \n        # Residuals\n        residuals = self.data - self.best_model.fittedvalues\n        residuals.plot(ax=axes[0,1], title='Residuals')\n        axes[0,1].axhline(y=0, color='red', linestyle='--')\n        axes[0,1].grid(True)\n        \n        # Level component\n        if hasattr(self.best_model, 'level'):\n            self.best_model.level.plot(ax=axes[1,0], title='Level Component', color='green')\n        axes[1,0].grid(True)\n        \n        # Trend component\n        if hasattr(self.best_model, 'trend') and self.best_model.trend is not None:\n            self.best_model.trend.plot(ax=axes[1,1], title='Trend Component', color='orange')\n            axes[1,1].grid(True)\n        else:\n            axes[1,1].text(0.5, 0.5, 'No Trend Component', ha='center', va='center', transform=axes[1,1].transAxes)\n        \n        # Seasonal component\n        if hasattr(self.best_model, 'season') and self.best_model.season is not None:\n            seasonal_component = self.best_model.season\n            seasonal_component.plot(ax=axes[2,0], title='Seasonal Component', color='purple')\n            axes[2,0].grid(True)\n            \n            # Seasonal indices\n            seasonal_indices = seasonal_component.groupby(seasonal_component.index.month).mean()\n            seasonal_indices.plot(ax=axes[2,1], kind='bar', title='Average Seasonal Indices')\n            axes[2,1].tick_params(axis='x', rotation=0)\n        else:\n            axes[2,0].text(0.5, 0.5, 'No Seasonal Component', ha='center', va='center', transform=axes[2,0].transAxes)\n            axes[2,1].text(0.5, 0.5, 'No Seasonal Component', ha='center', va='center', transform=axes[2,1].transAxes)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Parameter summary\n        print(\"\\n=== Model Parameters ===\")\n        if hasattr(self.best_model, 'params'):\n            params = self.best_model.params\n            if 'smoothing_level' in params:\n                print(f\"α (Level smoothing): {params['smoothing_level']:.4f}\")\n            if 'smoothing_trend' in params:\n                print(f\"β (Trend smoothing): {params['smoothing_trend']:.4f}\")\n            if 'smoothing_seasonal' in params:\n                print(f\"γ (Seasonal smoothing): {params['smoothing_seasonal']:.4f}\")\n            if 'damping_trend' in params:\n                print(f\"φ (Trend damping): {params['damping_trend']:.4f}\")\n    \n    def forecast_analysis(self, forecast_periods=12):\n        \"\"\"Generate and analyze forecasts\"\"\"\n        if not hasattr(self, 'best_model'):\n            raise ValueError(\"Must fit models first using fit_all_models()\")\n        \n        # Generate forecasts\n        forecast = self.best_model.forecast(steps=forecast_periods)\n        \n        # Prediction intervals (approximate)\n        residuals = self.data - self.best_model.fittedvalues\n        residual_std = residuals.std()\n        \n        # Simple prediction intervals (can be improved with proper calculation)\n        forecast_index = pd.date_range(start=self.data.index[-1] + pd.DateOffset(months=1), \n                                     periods=forecast_periods, freq='M')\n        forecast_series = pd.Series(forecast, index=forecast_index)\n        \n        # Forecast intervals (simplified - real implementation would use proper statistical methods)\n        forecast_upper = forecast_series + 1.96 * residual_std * np.sqrt(range(1, forecast_periods + 1))\n        forecast_lower = forecast_series - 1.96 * residual_std * np.sqrt(range(1, forecast_periods + 1))\n        \n        # Plot forecast\n        fig, ax = plt.subplots(figsize=(15, 8))\n        \n        # Historical data (last 3 years)\n        historical_subset = self.data[-36:] if len(self.data) > 36 else self.data\n        historical_subset.plot(ax=ax, label='Historical', color='blue', linewidth=2)\n        \n        # Forecast\n        forecast_series.plot(ax=ax, label='Forecast', color='red', linewidth=2)\n        \n        # Prediction intervals\n        ax.fill_between(forecast_series.index, forecast_lower, forecast_upper, \n                       color='red', alpha=0.2, label='95% Prediction Interval')\n        \n        ax.set_title(f'{forecast_periods}-Period Forecast using Best Holt-Winters Model')\n        ax.legend()\n        ax.grid(True)\n        plt.tight_layout()\n        plt.show()\n        \n        # Forecast summary\n        print(f\"\\n=== {forecast_periods}-Period Forecast Summary ===\")\n        print(f\"Average forecasted value: {forecast_series.mean():.2f}\")\n        print(f\"Forecasted growth rate: {((forecast_series.iloc[-1] / self.data.iloc[-1]) ** (1/forecast_periods) - 1) * 100:.2f}% per period\")\n        print(f\"Peak forecasted value: {forecast_series.max():.2f} (Period {forecast_series.idxmax().strftime('%Y-%m')})\")\n        print(f\"Low forecasted value: {forecast_series.min():.2f} (Period {forecast_series.idxmin().strftime('%Y-%m')})\")\n        \n        if hasattr(self.best_model, 'season') and self.best_model.season is not None:\n            seasonal_pattern = forecast_series.groupby(forecast_series.index.month).mean()\n            peak_month = seasonal_pattern.idxmax()\n            low_month = seasonal_pattern.idxmin()\n            print(f\"Seasonal peaks typically in: {pd.Timestamp(2000, peak_month, 1).strftime('%B')}\")\n            print(f\"Seasonal lows typically in: {pd.Timestamp(2000, low_month, 1).strftime('%B')}\")\n        \n        return forecast_series, forecast_lower, forecast_upper\n    \n    def cross_validate(self, initial_train_size=24, horizon=6, step=1):\n        \"\"\"Time series cross-validation\"\"\"\n        errors = []\n        \n        for start_idx in range(initial_train_size, len(self.data) - horizon + 1, step):\n            # Split data\n            train_data = self.data.iloc[:start_idx]\n            test_data = self.data.iloc[start_idx:start_idx + horizon]\n            \n            try:\n                # Fit model on training data\n                model = ExponentialSmoothing(\n                    train_data,\n                    trend='add' if hasattr(self.best_model, 'trend') and self.best_model.trend is not None else None,\n                    seasonal='add' if hasattr(self.best_model, 'season') and self.best_model.season is not None else None,\n                    seasonal_periods=12\n                )\n                fitted = model.fit()\n                \n                # Forecast\n                forecast = fitted.forecast(steps=len(test_data))\n                \n                # Calculate errors\n                mape = np.mean(np.abs((test_data - forecast) / test_data)) * 100\n                mae = mean_absolute_error(test_data, forecast)\n                rmse = np.sqrt(mean_squared_error(test_data, forecast))\n                \n                errors.append({\n                    'Start': train_data.index[-1],\n                    'End': test_data.index[-1],\n                    'MAPE': mape,\n                    'MAE': mae,\n                    'RMSE': rmse\n                })\n                \n            except Exception as e:\n                print(f\"CV fold failed for period {train_data.index[-1]} to {test_data.index[-1]}: {e}\")\n        \n        cv_results = pd.DataFrame(errors)\n        \n        print(\"\\n=== Cross-Validation Results ===\")\n        print(f\"Average MAPE: {cv_results['MAPE'].mean():.2f}% (±{cv_results['MAPE'].std():.2f})\")\n        print(f\"Average MAE: {cv_results['MAE'].mean():.2f} (±{cv_results['MAE'].std():.2f})\")\n        print(f\"Average RMSE: {cv_results['RMSE'].mean():.2f} (±{cv_results['RMSE'].std():.2f})\")\n        \n        return cv_results\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize analyzer\n    analyzer = HoltWintersAnalyzer(ts)\n    \n    # Step 1: Explore seasonality\n    print(\"Step 1: Analyzing seasonality patterns...\")\n    seasonal_strength_add, seasonal_strength_mult = analyzer.explore_seasonality()\n    \n    # Step 2: Fit all models\n    print(\"\\nStep 2: Fitting all ETS models...\")\n    results_df = analyzer.fit_all_models()\n    \n    # Step 3: Analyze best model components\n    print(\"\\nStep 3: Analyzing model components...\")\n    analyzer.analyze_components()\n    \n    # Step 4: Generate forecasts\n    print(\"\\nStep 4: Generating forecasts...\")\n    forecast_series, forecast_lower, forecast_upper = analyzer.forecast_analysis(forecast_periods=18)\n    \n    # Step 5: Cross-validation\n    print(\"\\nStep 5: Performing cross-validation...\")\n    cv_results = analyzer.cross_validate()\n    \n    print(\"\\n=== Analysis Complete ===\")\n    print(f\"Best model selected: {results_df.iloc[0]['Model']}\")\n    print(f\"Model AIC: {results_df.iloc[0]['AIC']:.2f}\")\n    print(f\"Cross-validated MAPE: {cv_results['MAPE'].mean():.2f}%\")\n```"
        },
        {
          "type": "case_study",
          "title": "Manufacturing Demand Planning Case Study",
          "content": "**Challenge**: Global automotive parts manufacturer needs accurate demand forecasts for 500+ components across multiple plants with complex seasonal patterns driven by car production cycles.\n\n**Data Characteristics:**\n• **Frequency**: Weekly demand data (52 observations per year)\n• **Seasonality**: Multiple overlapping patterns - quarterly model releases, summer shutdowns, holiday effects\n• **Volatility**: High variability due to just-in-time manufacturing\n• **External Factors**: Economic conditions, fuel prices, regulatory changes\n• **Lead Times**: 4-12 week supplier lead times requiring accurate forecasts\n\n**Holt-Winters Implementation:**\n\n**1. Component Segmentation:**\n```python\nclass ComponentForecaster:\n    def __init__(self):\n        self.component_categories = {\n            'engine_parts': {'seasonality': 'multiplicative', 'trend': 'damped'},\n            'body_parts': {'seasonality': 'additive', 'trend': 'linear'},\n            'electronics': {'seasonality': 'multiplicative', 'trend': 'linear'},\n            'interior': {'seasonality': 'additive', 'trend': 'damped'}\n        }\n    \n    def fit_category_model(self, data, category):\n        config = self.component_categories[category]\n        \n        model = ExponentialSmoothing(\n            data,\n            trend='add_damped' if config['trend'] == 'damped' else 'add',\n            seasonal=config['seasonality'],\n            seasonal_periods=52,  # Weekly seasonality\n            damped_trend=(config['trend'] == 'damped')\n        )\n        \n        return model.fit(optimized=True)\n```\n\n**2. Hierarchical Forecasting:**\n• **Top-Level**: Total plant demand using Holt-Winters\n• **Middle-Level**: Product family forecasts with constraints\n• **Bottom-Level**: Individual component forecasts\n• **Reconciliation**: Ensure forecasts sum consistently across hierarchy\n\n**3. Business Results:**\n\n**Forecast Accuracy Improvements:**\n• **Engine Parts**: 18% MAPE improvement over naive seasonal\n• **Body Parts**: 22% MAPE improvement (additive seasonality optimal)\n• **Electronics**: 15% MAPE improvement (multiplicative seasonality)\n• **Interior**: 25% MAPE improvement (damped trend captured maturity)\n\n**Operational Impact:**\n• **Inventory Reduction**: $4.2M reduction in safety stock\n• **Service Level**: 98.5% fill rate maintained (vs 96% with old methods)\n• **Production Planning**: 30% reduction in emergency changeovers\n• **Supplier Relations**: 40% reduction in expediting costs\n\n**4. Key Implementation Insights:**\n\n**Model Selection by Component Type:**\n• **High-Volume Commodities**: Simple exponential smoothing sufficient\n• **Growing Categories**: Linear trend Holt-Winters\n• **Mature Products**: Damped trend prevents over-forecasting\n• **Fashion Items**: High seasonal smoothing parameter (γ=0.8)\n\n**Seasonal Pattern Analysis:**\n• **Q1**: Post-holiday production ramp-up (15% below average)\n• **Q2**: Peak production for summer models (20% above average)\n• **Q3**: Summer shutdown impact (10% below average)\n• **Q4**: Year-end push and new model prep (variable)\n\n**Parameter Optimization Results:**\n```python\n# Optimal parameters by category (averaged)\noptimal_params = {\n    'engine_parts': {'alpha': 0.3, 'beta': 0.1, 'gamma': 0.4, 'phi': 0.8},\n    'body_parts': {'alpha': 0.4, 'beta': 0.2, 'gamma': 0.3, 'phi': None},\n    'electronics': {'alpha': 0.5, 'beta': 0.3, 'gamma': 0.6, 'phi': None},\n    'interior': {'alpha': 0.3, 'beta': 0.1, 'gamma': 0.2, 'phi': 0.9}\n}\n```\n\n**5. Advanced Enhancements:**\n\n**External Regressor Integration:**\n• **Economic Indicators**: GDP growth, industrial production\n• **Industry Metrics**: Vehicle production forecasts, inventory levels\n• **Calendar Effects**: Holiday adjustments, shutdown schedules\n\n**Forecast Combination:**\n• **Ensemble Methods**: Combine Holt-Winters with ARIMA and ML models\n• **Weighted Averaging**: Performance-based model weights\n• **Scenario Planning**: Best/worst case demand scenarios\n\n**Real-Time Updates:**\n• **Weekly Refresh**: Models re-estimated with new demand data\n• **Exception Monitoring**: Automatic alerts for forecast accuracy degradation\n• **Parameter Drift**: Tracking of smoothing parameters over time"
        },
        {
          "type": "mathematical",
          "title": "Advanced Exponential Smoothing Extensions",
          "content": "**Complex Seasonalities:**\n\n**Multiple Seasonal Periods:**\nFor data with nested seasonality (e.g., hourly data with daily + weekly patterns):\n\n**Double Seasonal Holt-Winters:**\n• **Level**: ℓₜ = α(xₜ/(sₜ₋ₘ₁ × sₜ₋ₘ₂)) + (1-α)(ℓₜ₋₁ + bₜ₋₁)\n• **Trend**: bₜ = β(ℓₜ - ℓₜ₋₁) + (1-β)bₜ₋₁\n• **Seasonal 1**: s¹ₜ = γ₁(xₜ/(ℓₜ × sₜ₋ₘ₂)) + (1-γ₁)s¹ₜ₋ₘ₁\n• **Seasonal 2**: s²ₜ = γ₂(xₜ/(ℓₜ × s¹ₜ₋ₘ₁)) + (1-γ₂)s²ₜ₋ₘ₂\n\n**State Space Formulation:**\n\n**Innovation State Space Models:**\n**Measurement equation**: yₜ = h(xₜ₋₁) + ε₊ₜ\n**State equations**: xₜ = f(xₜ₋₁) + g(xₜ₋₁)εₜ\n\nWhere:\n• **h()**: Observation function\n• **f()**: State transition function  \n• **g()**: Error impact function\n• **εₜ**: Innovation process\n\n**ETS Error Types:**\n• **Additive**: εₜ ~ N(0, σ²)\n• **Multiplicative**: εₜ ~ N(1, σ²/μ²ₜ)\n\n**Prediction Intervals:**\n\nFor additive errors:\n**h-step ahead**: ŷₜ₊ₕ ± z_{α/2}σ√(1 + ∑ᵢ₌₁ʰ gᵢ²)\n\nFor multiplicative errors:\n**h-step ahead**: ŷₜ₊ₕ × (1 ± z_{α/2}σ/μₜ₊ₕ√(1 + ∑ᵢ₌₁ʰ gᵢ²))\n\n**Bayesian Exponential Smoothing:**\n\n**Prior Distributions:**\n• **Smoothing parameters**: α, β, γ ~ Beta(a, b)\n• **Initial states**: ℓ₀, b₀ ~ N(μ, Σ)\n• **Error variance**: σ² ~ InvGamma(ν, S)\n\n**Posterior Sampling:**\n1. **Gibbs Sampling**: Alternate between parameter and state updates\n2. **Metropolis-Hastings**: For complex posteriors\n3. **Variational Bayes**: Approximate posterior inference\n\n**Model Averaging:**\n\n**Bayesian Model Averaging (BMA):**\nP(yₜ₊ₕ|data) = ∑ₖ P(yₜ₊ₕ|Mₖ, data) × P(Mₖ|data)\n\nWhere:\n• **P(Mₖ|data)**: Posterior model probability\n• **P(yₜ₊ₕ|Mₖ, data)**: Model-specific forecast\n\n**Information Criteria Weights:**\nwᵢ = exp(-0.5 × ΔAICᵢ) / ∑ⱼ exp(-0.5 × ΔAICⱼ)\n\n**Robust Exponential Smoothing:**\n\n**M-estimators**: Replace least squares with robust loss functions\n• **Huber Loss**: ρ(e) = e²/2 if |e| ≤ k, k|e| - k²/2 if |e| > k\n• **Tukey's Biweight**: Bounded influence function\n\n**Outlier Detection**: Automatic identification and downweighting of anomalous observations"
        }
      ],
      "quiz": {
        "questions": [
          {
            "id": 1,
            "type": "multiple_choice",
            "question": "In Holt-Winters multiplicative seasonality, what does a seasonal index of 1.2 for December mean?",
            "options": [
              "December values are 20% above the deseasonalized level",
              "December has 1.2 times more observations",
              "December forecast has 20% confidence",
              "December trend is 1.2 units higher"
            ],
            "correct_answer": 0,
            "explanation": "A multiplicative seasonal index of 1.2 means December values are typically 20% higher than the deseasonalized baseline level (1.2 × baseline)."
          },
          {
            "id": 2,
            "type": "multiple_choice",
            "question": "What is the key advantage of damped trend Holt-Winters over linear trend?",
            "options": [
              "Faster computation",
              "Better seasonal modeling",
              "Prevents unrealistic long-term growth",
              "Handles missing data better"
            ],
            "correct_answer": 2,
            "explanation": "Damped trend methods apply a damping parameter φ < 1 that causes the trend to level off over time, preventing unrealistic exponential growth in long-term forecasts."
          },
          {
            "id": 3,
            "type": "true_false",
            "question": "Higher values of the smoothing parameter α make exponential smoothing more responsive to recent changes.",
            "correct_answer": true,
            "explanation": "Higher α values (closer to 1) give more weight to recent observations, making the model more responsive to recent changes but potentially less stable."
          },
          {
            "id": 4,
            "type": "coding",
            "question": "Write Python code using statsmodels to fit a Holt-Winters model with additive trend and multiplicative seasonality to a pandas Series 'ts', then forecast 6 periods ahead.",
            "correct_answer": "from statsmodels.tsa.holtwinters import ExponentialSmoothing\nmodel = ExponentialSmoothing(ts, trend='add', seasonal='mul', seasonal_periods=12)\nfitted_model = model.fit()\nforecast = fitted_model.forecast(steps=6)",
            "explanation": "ExponentialSmoothing with trend='add' and seasonal='mul' creates a Holt-Winters model with additive trend and multiplicative seasonality. The seasonal_periods parameter specifies the seasonal cycle length."
          },
          {
            "id": 4,
            "type": "multiple_choice",
            "question": "What is the primary purpose of the damping parameter (φ) in Holt-Winters damped trend method?",
            "options": [
              "To reduce seasonal fluctuations",
              "To flatten the trend over time, preventing infinite growth",
              "To smooth the level component",
              "To handle missing values"
            ],
            "correct_answer": 1,
            "explanation": "The damping parameter φ (0 < φ ≤ 1) reduces the impact of the trend component as forecasts extend further into the future, preventing unrealistic linear growth and providing more conservative long-term forecasts."
          },
          {
            "id": 5,
            "type": "true_false",
            "question": "Holt-Winters additive seasonality is more appropriate than multiplicative when seasonal fluctuations remain constant regardless of the series level.",
            "correct_answer": true,
            "explanation": "Additive seasonality assumes seasonal variations are constant in absolute terms. Use multiplicative seasonality when seasonal amplitude increases proportionally with the level of the series."
          },
          {
            "id": 6,
            "type": "multiple_choice",
            "question": "In ETS(M,A,M) notation, what do the three letters represent?",
            "options": [
              "Model, Additive, Multiplicative",
              "Error type, Trend type, Seasonal type",
              "Mean, Autoregressive, Moving average",
              "Monthly, Annual, Maximum"
            ],
            "correct_answer": 1,
            "explanation": "ETS notation specifies Error type (Additive/Multiplicative), Trend type (None/Additive/Damped), and Seasonal type (None/Additive/Multiplicative). ETS(M,A,M) has Multiplicative errors, Additive trend, and Multiplicative seasonality."
          },
          {
            "id": 7,
            "type": "coding",
            "question": "Write code to fit a Holt-Winters model with additive trend, multiplicative seasonality, and damped trend to monthly data in pandas Series 'sales', then forecast 6 periods ahead.",
            "correct_answer": "from statsmodels.tsa.holtwinters import ExponentialSmoothing\nmodel = ExponentialSmoothing(sales, trend='add', seasonal='mul', seasonal_periods=12, damped_trend=True)\nfitted_model = model.fit()\nforecast = fitted_model.forecast(steps=6)",
            "explanation": "ExponentialSmoothing with trend='add', seasonal='mul', damped_trend=True, and seasonal_periods=12 creates the required model. The fit() method estimates parameters, and forecast() generates future predictions."
          }
        ]
      }
    },
    {
      "id": "prophet-models",
      "title": "Prophet: Modern Forecasting at Scale",
      "description": "Master Facebook's Prophet framework for scalable time series forecasting with automatic seasonality detection, holiday effects, and robust trend modeling.",
      "duration": "85 minutes",
      "difficulty": "Advanced",
      "content": [
        {
          "type": "introduction",
          "title": "Prophet: Forecasting for Everyone",
          "content": "Prophet revolutionizes time series forecasting by making advanced techniques accessible to analysts without deep statistical expertise. Developed by Facebook's data science team, Prophet handles the complexities of real-world time series: missing data, outliers, seasonal changes, and holiday effects with minimal manual intervention.\n\n**Key Innovations:**\n• **Automatic Seasonality**: Detects yearly, weekly, and daily patterns\n• **Holiday Integration**: Built-in support for country-specific holidays\n• **Trend Flexibility**: Automatic changepoint detection for trend shifts\n• **Robust to Outliers**: Resistant to anomalous data points\n• **Intuitive Parameters**: Business-friendly parameter interpretation\n• **Uncertainty Quantification**: Realistic prediction intervals\n\n**Business Applications:**\n• **Digital Platforms**: User engagement, content consumption, ad performance\n• **E-commerce**: Sales forecasting, inventory planning, promotional impact\n• **Finance**: Revenue projections, risk modeling, budget planning\n• **Operations**: Capacity planning, resource allocation, demand sensing\n• **Energy**: Load forecasting, renewable generation, consumption patterns"
        },
        {
          "type": "mathematical",
          "title": "Prophet Mathematical Framework",
          "content": "**Additive Time Series Decomposition:**\n\ny(t) = g(t) + s(t) + h(t) + εₜ\n\nWhere:\n• **g(t)**: Piecewise linear or logistic growth trend\n• **s(t)**: Periodic seasonality (Fourier series)\n• **h(t)**: Holiday and special event effects\n• **εₜ**: Error term (normally distributed)\n\n**Trend Component g(t):**\n\n**1. Linear Growth:**\ng(t) = (k + a(t)ᵀδ) × t + (m + a(t)ᵀγ)\n\n• **k**: Base growth rate\n• **δⱼ**: Growth rate adjustments at changepoints\n• **m**: Offset parameter\n• **γⱼ**: Offset adjustments at changepoints\n• **a(t)**: Indicator function for changepoint activation\n\n**2. Logistic Growth:**\ng(t) = C(t) / (1 + exp(-k×t + b))\n\n• **C(t)**: Time-varying carrying capacity\n• **k**: Growth rate parameter\n• **b**: Offset parameter\n\n**Seasonality Component s(t):**\n\nUsing Fourier series for flexible seasonal patterns:\ns(t) = Σₙ₌₁ᴺ [aₙcos(2πnt/P) + bₙsin(2πnt/P)]\n\n• **P**: Period (365.25 for yearly, 7 for weekly)\n• **N**: Number of Fourier terms (controls flexibility)\n• **aₙ, bₙ**: Fourier coefficients (fitted parameters)\n\n**Multiple Seasonalities:**\n• **Yearly**: N=10 Fourier terms (captures complex yearly patterns)\n• **Weekly**: N=3 Fourier terms (weekly cycles)\n• **Daily**: N=4 Fourier terms (intraday patterns)\n\n**Holiday Effects h(t):**\n\nFor holiday i occurring on day d:\nh(t) = κᵢ × (1 if t ∈ [d - lower_window, d + upper_window], 0 otherwise)\n\n• **κᵢ**: Holiday effect magnitude\n• **lower_window, upper_window**: Days before/after holiday affected\n\n**Changepoint Detection:**\n\n**Automatic Detection**: Potential changepoints at S evenly spaced points\n**Sparse Prior**: δⱼ ~ Laplace(0, τ) encourages few actual changes\n**Bayesian Selection**: Posterior probability determines active changepoints\n\n**Uncertainty Quantification:**\n\n**1. Trend Uncertainty**: Monte Carlo simulation of trend parameters\n**2. Seasonal Uncertainty**: Bootstrap resampling of seasonal estimates  \n**3. Observation Noise**: σ² from residual analysis\n**4. Combined Intervals**: yₜ₊ₕ ± z_{α/2}√(σ²trend + σ²seasonal + σ²noise)"
        },
        {
          "type": "practical",
          "title": "Prophet Implementation Best Practices",
          "content": "**Data Preparation for Prophet:**\n\n**Required Format:**\n• **ds column**: Date/datetime (pandas datetime or string)\n• **y column**: Target variable (numeric)\n• **Frequency**: Daily or higher resolution\n• **Missing Data**: Prophet handles gaps automatically\n• **Outliers**: Robust fitting, but extreme outliers should be investigated\n\n**Data Quality Checklist:**\n```python\n# 1. Date formatting\ndf['ds'] = pd.to_datetime(df['ds'])\n\n# 2. Remove duplicates\ndf = df.drop_duplicates(subset=['ds']).sort_values('ds')\n\n# 3. Handle outliers (optional)\nQ1, Q3 = df['y'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\ndf.loc[(df['y'] < lower_bound) | (df['y'] > upper_bound), 'y'] = np.nan\n\n# 4. Check frequency\nprint(f\"Date range: {df['ds'].min()} to {df['ds'].max()}\")\nprint(f\"Frequency: {pd.infer_freq(df['ds'])}\")\n```\n\n**Model Configuration:**\n\n**1. Growth Parameters:**\n```python\n# Linear growth (default)\nmodel = Prophet()\n\n# Logistic growth with capacity\ndf['cap'] = 1000000  # Set capacity\nmodel = Prophet(growth='logistic')\n\n# Flat trend (no growth)\nmodel = Prophet(growth='flat')\n```\n\n**2. Seasonality Control:**\n```python\nmodel = Prophet(\n    yearly_seasonality=True,    # Automatic yearly patterns\n    weekly_seasonality=True,    # Weekly cycles  \n    daily_seasonality=False,    # Disable if not daily data\n    seasonality_mode='multiplicative'  # vs 'additive'\n)\n\n# Custom seasonality\nmodel.add_seasonality(\n    name='monthly',\n    period=30.5,\n    fourier_order=5\n)\n```\n\n**3. Holiday Configuration:**\n```python\n# Built-in country holidays\nmodel = Prophet(holidays=holidays_df)\n\n# Custom holidays\ncustom_holidays = pd.DataFrame({\n    'holiday': 'black_friday',\n    'ds': pd.to_datetime(['2019-11-29', '2020-11-27', '2021-11-26']),\n    'lower_window': -1,  # 1 day before\n    'upper_window': 2,   # 2 days after\n})\n\nmodel = Prophet(holidays=custom_holidays)\n```\n\n**4. Changepoint Tuning:**\n```python\nmodel = Prophet(\n    changepoint_prior_scale=0.05,    # Default: 0.05 (higher = more flexible)\n    n_changepoints=25,               # Number of potential changepoints\n    changepoint_range=0.8,           # Fraction of data for changepoints\n    changepoints=['2020-03-15', '2021-01-01']  # Manual specification\n)\n```\n\n**Parameter Guidelines:**\n\n**changepoint_prior_scale:**\n• **0.001-0.01**: Conservative (smooth trends)\n• **0.05**: Default (balanced)\n• **0.1-0.5**: Flexible (captures trend changes)\n• **>0.5**: Very flexible (risk of overfitting)\n\n**seasonality_prior_scale:**\n• **0.01**: Weak seasonality\n• **10**: Default (moderate seasonality)\n• **50-100**: Strong seasonal patterns\n\n**holidays_prior_scale:**\n• **1**: Weak holiday effects\n• **10**: Default (moderate effects)\n• **50**: Strong holiday impacts\n\n**Model Diagnostics:**\n\n**1. Cross-Validation:**\n```python\nfrom prophet.diagnostics import cross_validation, performance_metrics\n\n# Time series cross-validation\ncv_results = cross_validation(\n    model, \n    initial='730 days',    # Initial training period\n    period='180 days',     # Frequency of forecasts\n    horizon='365 days'     # Forecast horizon\n)\n\n# Performance metrics\nmetrics = performance_metrics(cv_results)\nprint(metrics[['mape', 'rmse', 'mae']].mean())\n```\n\n**2. Component Analysis:**\n```python\n# Decompose forecast into components\nforecast = model.predict(future)\nfig = model.plot_components(forecast)\n\n# Custom component plots\nfrom prophet.plot import plot_seasonality\nfig = plot_seasonality(model, 'yearly')\n```\n\n**3. Residual Analysis:**\n```python\n# In-sample residuals\nresiduals = df['y'] - forecast.loc[forecast['ds'].isin(df['ds']), 'yhat']\n\n# Residual plots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nresiduals.plot(ax=axes[0,0], title='Residuals Over Time')\nresiduals.hist(ax=axes[0,1], bins=30, title='Residual Distribution')\nstats.probplot(residuals.dropna(), dist=\"norm\", plot=axes[1,0])\naxes[1,0].set_title('Q-Q Plot')\npd.plotting.lag_plot(residuals.dropna(), ax=axes[1,1])\naxes[1,1].set_title('Lag Plot')\n```"
        },
        {
          "type": "code_example",
          "title": "Complete Prophet Implementation",
          "content": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\nfrom prophet.diagnostics import cross_validation, performance_metrics\nfrom prophet.plot import plot_cross_validation_metric, plot_seasonality\nimport holidays\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ProphetAnalyzer:\n    def __init__(self, data, date_col='ds', target_col='y'):\n        \"\"\"Initialize Prophet analyzer with time series data\"\"\"\n        self.data = data.copy()\n        self.date_col = date_col\n        self.target_col = target_col\n        self.model = None\n        self.forecast = None\n        self.cv_results = None\n        \n        # Prepare data\n        self._prepare_data()\n    \n    def _prepare_data(self):\n        \"\"\"Prepare data for Prophet\"\"\"\n        print(\"📊 Preparing data for Prophet...\")\n        \n        # Ensure correct column names\n        if self.date_col != 'ds':\n            self.data = self.data.rename(columns={self.date_col: 'ds'})\n        if self.target_col != 'y':\n            self.data = self.data.rename(columns={self.target_col: 'y'})\n        \n        # Convert to datetime\n        self.data['ds'] = pd.to_datetime(self.data['ds'])\n        \n        # Sort by date\n        self.data = self.data.sort_values('ds').reset_index(drop=True)\n        \n        # Remove duplicates\n        original_len = len(self.data)\n        self.data = self.data.drop_duplicates(subset=['ds'])\n        if len(self.data) < original_len:\n            print(f\"⚠️ Removed {original_len - len(self.data)} duplicate dates\")\n        \n        # Basic statistics\n        print(f\"📈 Data summary:\")\n        print(f\"   Date range: {self.data['ds'].min().date()} to {self.data['ds'].max().date()}\")\n        print(f\"   Total observations: {len(self.data)}\")\n        print(f\"   Missing values: {self.data['y'].isna().sum()}\")\n        print(f\"   Data frequency: {pd.infer_freq(self.data['ds']) or 'Irregular'}\")\n    \n    def explore_patterns(self):\n        \"\"\"Explore data patterns and characteristics\"\"\"\n        fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n        \n        # Time series plot\n        self.data.set_index('ds')['y'].plot(ax=axes[0,0], title='Time Series')\n        axes[0,0].grid(True)\n        \n        # Seasonal patterns\n        # Yearly pattern\n        yearly_pattern = self.data.copy()\n        yearly_pattern['month'] = yearly_pattern['ds'].dt.month\n        monthly_avg = yearly_pattern.groupby('month')['y'].mean()\n        monthly_avg.plot(kind='bar', ax=axes[0,1], title='Average by Month')\n        axes[0,1].tick_params(axis='x', rotation=0)\n        \n        # Weekly pattern (if daily data)\n        if len(self.data) > 14:  # At least 2 weeks of data\n            weekly_pattern = self.data.copy()\n            weekly_pattern['weekday'] = weekly_pattern['ds'].dt.dayofweek\n            weekday_avg = weekly_pattern.groupby('weekday')['y'].mean()\n            weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n            weekday_avg.index = weekday_names\n            weekday_avg.plot(kind='bar', ax=axes[1,0], title='Average by Weekday')\n            axes[1,0].tick_params(axis='x', rotation=45)\n        else:\n            axes[1,0].text(0.5, 0.5, 'Insufficient data\\nfor weekly analysis', \n                          ha='center', va='center', transform=axes[1,0].transAxes)\n        \n        # Distribution\n        self.data['y'].hist(bins=30, ax=axes[1,1], title='Value Distribution')\n        axes[1,1].axvline(self.data['y'].mean(), color='red', linestyle='--', label='Mean')\n        axes[1,1].axvline(self.data['y'].median(), color='orange', linestyle='--', label='Median')\n        axes[1,1].legend()\n        \n        # Growth analysis\n        growth_data = self.data.copy()\n        growth_data['year'] = growth_data['ds'].dt.year\n        yearly_avg = growth_data.groupby('year')['y'].mean()\n        if len(yearly_avg) > 1:\n            yearly_avg.plot(ax=axes[2,0], title='Yearly Averages', marker='o')\n            axes[2,0].grid(True)\n        else:\n            axes[2,0].text(0.5, 0.5, 'Insufficient data\\nfor yearly analysis', \n                          ha='center', va='center', transform=axes[2,0].transAxes)\n        \n        # Autocorrelation\n        from pandas.plotting import autocorrelation_plot\n        autocorrelation_plot(self.data.set_index('ds')['y'], ax=axes[2,1])\n        axes[2,1].set_title('Autocorrelation')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Statistical summary\n        print(\"\\n📊 Statistical Summary:\")\n        print(self.data['y'].describe())\n        \n        # Trend analysis\n        first_year = self.data['y'][:min(365, len(self.data)//3)].mean()\n        last_year = self.data['y'][-min(365, len(self.data)//3):].mean()\n        trend_change = (last_year - first_year) / first_year * 100\n        print(f\"\\n📈 Trend Analysis:\")\n        print(f\"   Early period average: {first_year:.2f}\")\n        print(f\"   Recent period average: {last_year:.2f}\")\n        print(f\"   Overall trend: {trend_change:+.1f}%\")\n    \n    def fit_prophet_models(self, test_configurations=True):\n        \"\"\"Fit Prophet models with different configurations\"\"\"\n        print(\"\\n🚀 Fitting Prophet models...\")\n        \n        configurations = [\n            {\n                'name': 'Default Prophet',\n                'params': {}\n            },\n            {\n                'name': 'Multiplicative Seasonality',\n                'params': {'seasonality_mode': 'multiplicative'}\n            },\n            {\n                'name': 'Flexible Trend',\n                'params': {'changepoint_prior_scale': 0.1}\n            },\n            {\n                'name': 'Conservative Trend',\n                'params': {'changepoint_prior_scale': 0.01}\n            },\n            {\n                'name': 'Strong Seasonality',\n                'params': {'seasonality_prior_scale': 50}\n            }\n        ]\n        \n        # Add holidays if available\n        try:\n            # Try to detect country holidays (example for US)\n            us_holidays = holidays.US(years=range(self.data['ds'].dt.year.min(), \n                                                 self.data['ds'].dt.year.max() + 2))\n            holiday_df = pd.DataFrame({\n                'holiday': 'US_holiday',\n                'ds': pd.to_datetime(list(us_holidays.keys())),\n                'lower_window': 0,\n                'upper_window': 0,\n            })\n            \n            configurations.append({\n                'name': 'With US Holidays',\n                'params': {'holidays': holiday_df}\n            })\n            print(\"✅ US holidays added to model configurations\")\n        except:\n            print(\"⚠️ Could not add holidays - continuing without\")\n        \n        results = []\n        models = {}\n        \n        for config in configurations:\n            try:\n                print(f\"   Fitting: {config['name']}...\")\n                \n                # Create and fit model\n                model = Prophet(**config['params'])\n                model.fit(self.data)\n                \n                # Make forecast\n                future = model.make_future_dataframe(periods=30)  # 30 days ahead\n                forecast = model.predict(future)\n                \n                # Calculate in-sample metrics\n                train_forecast = forecast.loc[forecast['ds'].isin(self.data['ds'])]\n                \n                mae = mean_absolute_error(self.data['y'], train_forecast['yhat'])\n                rmse = np.sqrt(mean_squared_error(self.data['y'], train_forecast['yhat']))\n                mape = np.mean(np.abs((self.data['y'] - train_forecast['yhat']) / self.data['y'])) * 100\n                \n                results.append({\n                    'Model': config['name'],\n                    'MAE': mae,\n                    'RMSE': rmse,\n                    'MAPE': mape,\n                    'AIC': None  # Prophet doesn't provide AIC directly\n                })\n                \n                models[config['name']] = {'model': model, 'forecast': forecast}\n                \n            except Exception as e:\n                print(f\"   ❌ Failed to fit {config['name']}: {str(e)}\")\n        \n        # Results summary\n        results_df = pd.DataFrame(results)\n        print(\"\\n📊 Model Comparison Results:\")\n        print(results_df.round(3))\n        \n        # Select best model (lowest MAPE)\n        best_model_name = results_df.loc[results_df['MAPE'].idxmin(), 'Model']\n        self.model = models[best_model_name]['model']\n        self.forecast = models[best_model_name]['forecast']\n        \n        print(f\"\\n🏆 Best model: {best_model_name} (MAPE: {results_df['MAPE'].min():.2f}%)\")\n        \n        self.models = models\n        self.results_df = results_df\n        \n        return results_df\n    \n    def analyze_components(self):\n        \"\"\"Analyze model components\"\"\"\n        if self.model is None:\n            raise ValueError(\"Must fit model first!\")\n        \n        print(\"\\n🔍 Analyzing Prophet components...\")\n        \n        # Plot components\n        fig = self.model.plot_components(self.forecast, figsize=(16, 12))\n        plt.tight_layout()\n        plt.show()\n        \n        # Component statistics\n        components = ['trend', 'yearly', 'weekly']\n        available_components = [comp for comp in components if comp in self.forecast.columns]\n        \n        print(\"\\n📊 Component Analysis:\")\n        for comp in available_components:\n            component_data = self.forecast[comp].dropna()\n            if len(component_data) > 0:\n                print(f\"\\n{comp.title()} Component:\")\n                print(f\"   Range: {component_data.min():.2f} to {component_data.max():.2f}\")\n                print(f\"   Variation: {component_data.std():.2f}\")\n                \n                if comp == 'trend':\n                    # Trend analysis\n                    trend_change = (component_data.iloc[-1] - component_data.iloc[0]) / len(component_data)\n                    print(f\"   Daily trend change: {trend_change:.4f}\")\n                    print(f\"   Annualized growth: {trend_change * 365:.2f}\")\n        \n        # Changepoint analysis\n        if hasattr(self.model, 'changepoints'):\n            print(f\"\\n📍 Detected Changepoints: {len(self.model.changepoints)}\")\n            if len(self.model.changepoints) > 0:\n                print(\"   Most recent changepoints:\")\n                for cp in self.model.changepoints[-5:]:  # Last 5 changepoints\n                    print(f\"   - {cp.date()}\")\n    \n    def cross_validate_model(self, initial_days=365, period_days=90, horizon_days=180):\n        \"\"\"Perform time series cross-validation\"\"\"\n        if self.model is None:\n            raise ValueError(\"Must fit model first!\")\n        \n        print(f\"\\n🔄 Performing cross-validation...\")\n        print(f\"   Initial training: {initial_days} days\")\n        print(f\"   Evaluation period: {period_days} days\")\n        print(f\"   Forecast horizon: {horizon_days} days\")\n        \n        try:\n            self.cv_results = cross_validation(\n                self.model,\n                initial=f'{initial_days} days',\n                period=f'{period_days} days',\n                horizon=f'{horizon_days} days',\n                parallel=\"processes\"\n            )\n            \n            # Calculate performance metrics\n            metrics = performance_metrics(self.cv_results)\n            \n            print(\"\\n📊 Cross-Validation Results:\")\n            print(metrics[['mape', 'rmse', 'mae']].describe())\n            \n            # Plot cross-validation results\n            fig = plot_cross_validation_metric(self.cv_results, metric='mape', figsize=(12, 6))\n            plt.title('MAPE by Forecast Horizon')\n            plt.show()\n            \n            return metrics\n            \n        except Exception as e:\n            print(f\"❌ Cross-validation failed: {str(e)}\")\n            return None\n    \n    def generate_forecasts(self, periods=90, include_history=True):\n        \"\"\"Generate forecasts with uncertainty intervals\"\"\"\n        if self.model is None:\n            raise ValueError(\"Must fit model first!\")\n        \n        print(f\"\\n🔮 Generating {periods}-day forecast...\")\n        \n        # Create future dataframe\n        future = self.model.make_future_dataframe(periods=periods, include_history=include_history)\n        \n        # Generate forecast\n        forecast = self.model.predict(future)\n        \n        # Plot forecast\n        fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n        \n        # Main forecast plot\n        self.model.plot(forecast, ax=axes[0])\n        axes[0].set_title(f'{periods}-Day Prophet Forecast')\n        axes[0].grid(True)\n        \n        # Forecast uncertainty\n        forecast_only = forecast.tail(periods)\n        axes[1].plot(forecast_only['ds'], forecast_only['yhat'], 'r-', label='Forecast', linewidth=2)\n        axes[1].fill_between(\n            forecast_only['ds'],\n            forecast_only['yhat_lower'],\n            forecast_only['yhat_upper'],\n            alpha=0.3, color='red', label='80% Confidence Interval'\n        )\n        axes[1].set_title('Forecast Detail with Uncertainty')\n        axes[1].legend()\n        axes[1].grid(True)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Forecast summary\n        print(f\"\\n📈 Forecast Summary:\")\n        print(f\"   Forecast period: {forecast_only['ds'].min().date()} to {forecast_only['ds'].max().date()}\")\n        print(f\"   Average forecast: {forecast_only['yhat'].mean():.2f}\")\n        print(f\"   Forecast range: {forecast_only['yhat'].min():.2f} to {forecast_only['yhat'].max():.2f}\")\n        print(f\"   Growth over period: {(forecast_only['yhat'].iloc[-1] / self.data['y'].iloc[-1] - 1) * 100:+.1f}%\")\n        \n        # Seasonal forecast insights\n        if 'yearly' in forecast.columns:\n            seasonal_peak = forecast_only.loc[forecast_only['yearly'].idxmax(), 'ds']\n            seasonal_low = forecast_only.loc[forecast_only['yearly'].idxmin(), 'ds']\n            print(f\"   Seasonal peak expected: {seasonal_peak.strftime('%B %d')}\")\n            print(f\"   Seasonal low expected: {seasonal_low.strftime('%B %d')}\")\n        \n        return forecast\n\n# Usage example\nif __name__ == \"__main__\":\n    # Load and prepare data\n    data = pd.read_csv('your_data.csv')  # Replace with your data\n    data.columns = ['ds', 'y']  # Ensure correct column names\n    \n    # Initialize analyzer\n    analyzer = ProphetAnalyzer(data)\n    \n    # Step 1: Explore patterns\n    print(\"Step 1: Exploring data patterns...\")\n    analyzer.explore_patterns()\n    \n    # Step 2: Fit models\n    print(\"\\nStep 2: Fitting Prophet models...\")\n    results_df = analyzer.fit_prophet_models()\n    \n    # Step 3: Analyze components\n    print(\"\\nStep 3: Analyzing model components...\")\n    analyzer.analyze_components()\n    \n    # Step 4: Cross-validation\n    print(\"\\nStep 4: Cross-validation...\")\n    cv_metrics = analyzer.cross_validate_model()\n    \n    # Step 5: Generate forecasts\n    print(\"\\nStep 5: Generating forecasts...\")\n    forecast = analyzer.generate_forecasts(periods=60)\n    \n    print(\"\\n✅ Prophet analysis complete!\")\n```"
        },
        {
          "type": "case_study",
          "title": "Digital Marketing Attribution Case Study",
          "content": "**Challenge**: Global SaaS company needs to forecast user acquisition and revenue across multiple marketing channels while accounting for complex seasonality, promotional campaigns, and external market factors.\n\n**Data Complexity:**\n• **Multi-Channel Attribution**: Organic, paid search, social media, email, partnerships\n• **Seasonal Patterns**: B2B cycles (quarterly budgets), holiday effects, conference seasons\n• **Campaign Effects**: Product launches, promotional periods, competitive responses\n• **External Factors**: Economic conditions, industry events, platform algorithm changes\n• **Scale**: 5 years of daily data across 12 major markets\n\n**Prophet Implementation Strategy:**\n\n**1. Hierarchical Forecasting Architecture:**\n```python\nclass MarketingAttributionForecaster:\n    def __init__(self):\n        self.models = {}\n        self.forecasts = {}\n        self.channels = ['organic', 'paid_search', 'social', 'email', 'partnerships']\n        self.metrics = ['acquisitions', 'revenue', 'ltv']\n    \n    def prepare_marketing_data(self, df, channel, metric):\n        \"\"\"Prepare channel-specific data with regressors\"\"\"\n        data = df[df['channel'] == channel][['date', metric]].copy()\n        data.columns = ['ds', 'y']\n        \n        # Add regressors\n        data = self.add_marketing_regressors(data, channel)\n        return data\n    \n    def add_marketing_regressors(self, data, channel):\n        \"\"\"Add channel-specific external regressors\"\"\"\n        # Campaign indicators\n        data['campaign_active'] = self.get_campaign_periods(data['ds'], channel)\n        \n        # Competitive spend (when available)\n        data['competitor_index'] = self.get_competitive_intelligence(data['ds'])\n        \n        # Platform changes (for digital channels)\n        if channel in ['paid_search', 'social']:\n            data['algorithm_change'] = self.get_platform_updates(data['ds'], channel)\n        \n        # Economic indicators\n        data['gdp_growth'] = self.get_economic_data(data['ds'])\n        \n        return data\n```\n\n**2. Custom Seasonality and Holidays:**\n```python\ndef setup_b2b_seasonality(model):\n    \"\"\"Add B2B-specific seasonal patterns\"\"\"\n    # Quarterly business cycles\n    model.add_seasonality(\n        name='quarterly',\n        period=91.25,  # ~3 months\n        fourier_order=8,\n        condition_name='is_b2b_channel'\n    )\n    \n    # Conference season effects\n    model.add_seasonality(\n        name='conference_season',\n        period=365.25,\n        fourier_order=10,\n        condition_name='has_conferences'\n    )\n    \n    # Monthly budget cycles\n    model.add_seasonality(\n        name='monthly_budget',\n        period=30.5,\n        fourier_order=4\n    )\n\ndef create_marketing_holidays():\n    \"\"\"Define marketing-specific events\"\"\"\n    marketing_events = pd.DataFrame({\n        'holiday': [\n            'black_friday', 'cyber_monday', 'new_year_campaign',\n            'spring_conference', 'summer_lull', 'q4_push',\n            'product_launch_1', 'product_launch_2'\n        ],\n        'ds': pd.to_datetime([\n            '2023-11-24', '2023-11-27', '2024-01-01',\n            '2024-03-15', '2024-07-15', '2024-10-01',\n            '2023-09-15', '2024-02-15'\n        ]),\n        'lower_window': [-7, -3, -14, -5, -30, -15, -30, -30],\n        'upper_window': [3, 1, 7, 10, 15, 45, 60, 60]\n    })\n    return marketing_events\n```\n\n**3. Channel-Specific Model Configuration:**\n```python\nchannel_configs = {\n    'organic': {\n        'changepoint_prior_scale': 0.01,  # Stable, gradual changes\n        'seasonality_mode': 'multiplicative',\n        'yearly_seasonality': True,\n        'weekly_seasonality': True\n    },\n    'paid_search': {\n        'changepoint_prior_scale': 0.1,   # Responsive to algorithm changes\n        'seasonality_mode': 'additive',\n        'yearly_seasonality': True,\n        'weekly_seasonality': False  # Budget-driven, not day-of-week\n    },\n    'social': {\n        'changepoint_prior_scale': 0.2,   # Highly volatile\n        'seasonality_mode': 'multiplicative',\n        'yearly_seasonality': True,\n        'weekly_seasonality': True,  # Strong weekend patterns\n        'daily_seasonality': True    # Intraday posting patterns\n    },\n    'email': {\n        'changepoint_prior_scale': 0.05,  # Campaign-driven spikes\n        'seasonality_mode': 'additive',\n        'yearly_seasonality': True,\n        'weekly_seasonality': True   # Send day optimization\n    }\n}\n```\n\n**4. Business Results:**\n\n**Forecast Accuracy by Channel:**\n• **Organic Search**: 12% MAPE (stable, predictable patterns)\n• **Paid Search**: 18% MAPE (algorithm changes, competition)\n• **Social Media**: 25% MAPE (high volatility, viral effects)\n• **Email**: 15% MAPE (campaign-driven, controllable)\n• **Partnerships**: 22% MAPE (external dependencies)\n\n**Business Impact:**\n• **Budget Allocation**: $2.1M annual optimization through better channel forecasting\n• **Resource Planning**: 35% improvement in campaign timing decisions\n• **Inventory Management**: 20% reduction in stockouts during predicted demand spikes\n• **Executive Reporting**: Real-time dashboard with Prophet-powered projections\n\n**5. Advanced Implementation Features:**\n\n**Real-Time Model Updates:**\n```python\nclass RealTimeMarketingForecaster:\n    def __init__(self):\n        self.models = {}\n        self.last_update = {}\n        self.update_frequency = {\n            'paid_search': 'daily',    # High volatility\n            'social': 'daily',         # Platform changes\n            'organic': 'weekly',       # Stable patterns\n            'email': 'weekly'          # Campaign-based\n        }\n    \n    def should_retrain(self, channel):\n        \"\"\"Determine if model needs retraining\"\"\"\n        last_update = self.last_update.get(channel, datetime.min)\n        frequency = self.update_frequency[channel]\n        \n        if frequency == 'daily':\n            return (datetime.now() - last_update).days >= 1\n        elif frequency == 'weekly':\n            return (datetime.now() - last_update).days >= 7\n        \n        return False\n    \n    def detect_anomalies(self, actual, forecast, threshold=3):\n        \"\"\"Detect significant forecast errors for model updates\"\"\"\n        residuals = actual - forecast\n        z_scores = np.abs(residuals) / residuals.std()\n        return z_scores > threshold\n```\n\n**Campaign Impact Measurement:**\n```python\ndef measure_campaign_impact(model, forecast, campaign_dates):\n    \"\"\"Quantify campaign incrementality using Prophet\"\"\"\n    # Baseline forecast (without campaign)\n    baseline_model = Prophet()\n    baseline_data = data[~data['ds'].isin(campaign_dates)]\n    baseline_model.fit(baseline_data)\n    baseline_forecast = baseline_model.predict(future)\n    \n    # Actual forecast (with campaign)\n    actual_forecast = model.predict(future)\n    \n    # Calculate incrementality\n    incremental_impact = (\n        actual_forecast['yhat'].sum() - baseline_forecast['yhat'].sum()\n    )\n    \n    return {\n        'total_incremental': incremental_impact,\n        'daily_average': incremental_impact / len(campaign_dates),\n        'roi_multiplier': incremental_impact / campaign_cost\n    }\n```\n\n**6. Key Learnings and Best Practices:**\n\n**Prophet Advantages for Marketing:**\n• **Interpretability**: Marketing teams understand trend/seasonal components\n• **Holiday Handling**: Easy integration of campaign and event effects\n• **Robust to Missing Data**: Handles campaign gaps and data quality issues\n• **Flexible Seasonality**: Captures both B2B quarterly and B2C weekly patterns\n• **Uncertainty Quantification**: Provides confidence intervals for budget planning\n\n**Implementation Challenges:**\n• **External Regressors**: Prophet limited in incorporating competitive/economic data\n• **Cross-Channel Effects**: Individual channel models miss attribution complexities\n• **Short-Term Campaigns**: Prophet better for medium/long-term patterns\n• **Data Quality**: Requires consistent, high-quality historical data\n\n**Operational Integration:**\n• **Daily Dashboards**: Automated forecast updates with Prophet components\n• **Alert Systems**: Significant trend changes trigger stakeholder notifications\n• **Budget Planning**: Prophet forecasts feed into annual/quarterly planning\n• **A/B Testing**: Forecast baselines for measuring test incrementality"
        },
        {
          "type": "mathematical",
          "title": "Advanced Prophet Extensions and Theory",
          "content": "**Bayesian Inference Framework:**\n\nProphet employs Bayesian inference with the following hierarchical structure:\n\n**Likelihood:**\ny_t ~ Normal(g(t) + s(t) + h(t), σ²)\n\n**Priors:**\n• **Growth rate**: k ~ Normal(0, 5²)\n• **Changepoint effects**: δ_j ~ Laplace(0, τ)\n• **Seasonal coefficients**: β ~ Normal(0, σ²_s)\n• **Holiday effects**: κ ~ Normal(0, ν²)\n• **Noise variance**: σ² ~ InverseGamma(ν, σ)\n\n**Posterior Inference:**\nProphet uses L-BFGS optimization to find MAP estimates, then approximates uncertainty via:\n\n1. **Laplace Approximation**: Gaussian approximation around MAP\n2. **Bootstrap Sampling**: Empirical uncertainty from residuals\n3. **Monte Carlo**: Sample from posterior for complex predictions\n\n**Extended Seasonality Models:**\n\n**Conditional Seasonality:**\nFor business vs. leisure travel patterns:\ns(t) = s_business(t) × I_business(t) + s_leisure(t) × I_leisure(t)\n\n**Time-Varying Seasonality:**\nSeasonal patterns that evolve over time:\ns(t) = Σ_n [a_n(t)cos(2πnt/P) + b_n(t)sin(2πnt/P)]\n\nWhere coefficients a_n(t), b_n(t) follow random walks.\n\n**Multi-Resolution Seasonality:**\nNested seasonal patterns:\n• **Macro-seasonality**: Annual patterns (N=10 Fourier terms)\n• **Meso-seasonality**: Monthly patterns (N=5 terms)\n• **Micro-seasonality**: Weekly patterns (N=3 terms)\n\n**Advanced Trend Models:**\n\n**Smooth Trend Changes:**\nReplace piecewise linear with smooth transitions:\ng(t) = k × t + Σ_j δ_j × sigmoid((t - s_j)/λ)\n\n**Hierarchical Growth:**\nFor related time series (product families):\nk_i ~ Normal(μ_k, σ²_k)  # Individual growth rates\nμ_k ~ Normal(0, 10²)     # Group mean growth\n\n**Regime-Switching Trends:**\nDifferent growth regimes:\nk_t = k₁ × I(Z_t = 1) + k₂ × I(Z_t = 2)\n\nWhere Z_t follows a Markov chain.\n\n**Uncertainty Quantification:**\n\n**Predictive Intervals:**\nFor h-step ahead forecasts:\nP(y_{T+h} ∈ [L, U] | data) = ∫∫∫ P(y_{T+h} | θ, σ²) × p(θ, σ² | data) dθ dσ²\n\n**Components:**\n1. **Parameter Uncertainty**: Uncertainty in θ (trend, seasonality parameters)\n2. **Model Uncertainty**: Choice of seasonality, holidays, changepoints\n3. **Forecast Uncertainty**: Future innovations σ² × h\n\n**Empirical Prediction Intervals:**\nProphet approximates via simulation:\n1. Sample parameters from posterior approximation\n2. Generate future innovations from residual distribution\n3. Combine for full predictive distribution\n\n**Model Selection and Diagnostics:**\n\n**Information Criteria for Prophet:**\nWhile Prophet doesn't directly compute AIC/BIC, approximate via:\n- **Cross-Validation Error**: Time series CV performance\n- **Residual Analysis**: Ljung-Box tests for autocorrelation\n- **Component Significance**: Bootstrap confidence intervals\n\n**Residual Diagnostics:**\n```python\ndef prophet_diagnostics(model, forecast, actual):\n    residuals = actual - forecast['yhat']\n    \n    # Tests\n    jb_test = jarque_bera(residuals)          # Normality\n    lb_test = acorr_ljungbox(residuals, 10)   # Autocorrelation\n    arch_test = het_arch(residuals)           # Heteroscedasticity\n    \n    return {\n        'normality_pvalue': jb_test[1],\n        'autocorr_pvalue': lb_test['lb_pvalue'].min(),\n        'heterosced_pvalue': arch_test[1]\n    }\n```\n\n**Bayesian Model Averaging:**\nFor multiple Prophet specifications:\nP(y_{T+h} | data) = Σ_k P(y_{T+h} | M_k, data) × P(M_k | data)\n\n**Model Weights:**\nw_k = exp(-0.5 × CV_error_k) / Σ_j exp(-0.5 × CV_error_j)\n\n**Extensions and Variants:**\n\n**Multi-Task Prophet:**\nShared components across related time series:\n- **Shared Seasonality**: Common seasonal patterns\n- **Hierarchical Trends**: Related growth patterns\n- **Transfer Learning**: Use established series to improve new series forecasts\n\n**Prophet with Regressors:**\nExtend base model:\ny(t) = g(t) + s(t) + h(t) + Σ_k β_k × x_k(t) + ε_t\n\n**Limitations:**\n- Linear regressor effects only\n- No automatic regressor selection\n- Assumes stationary regressor relationships\n\n**Neural Prophet:**\nDeep learning extension combining:\n- **Prophet Decomposition**: Interpretable trend/seasonality\n- **Neural Networks**: Complex regressor relationships\n- **Autoregressive Components**: Lagged dependencies\n- **Attention Mechanisms**: Dynamic feature importance"
        }
      ],
      "quiz": {
        "questions": [
          {
            "id": 1,
            "type": "multiple_choice",
            "question": "What is the main advantage of Prophet's additive decomposition y(t) = g(t) + s(t) + h(t) + εₜ?",
            "options": [
              "Faster computation than multiplicative models",
              "Better handling of negative values",
              "Interpretable components that can be analyzed separately",
              "Automatic parameter selection"
            ],
            "correct_answer": 2,
            "explanation": "Prophet's additive decomposition allows each component (trend, seasonality, holidays) to be analyzed and interpreted independently, making it easier for business users to understand forecast drivers."
          },
          {
            "id": 2,
            "type": "multiple_choice",
            "question": "When should you use logistic growth instead of linear growth in Prophet?",
            "options": [
              "When data has strong seasonality",
              "When growth is expected to level off at a capacity limit",
              "When there are many outliers",
              "When you have external regressors"
            ],
            "correct_answer": 1,
            "explanation": "Logistic growth is appropriate when the time series is expected to reach a saturation point or capacity limit, such as market penetration or user adoption curves."
          },
          {
            "id": 3,
            "type": "true_false",
            "question": "Prophet automatically detects and handles changepoints in the trend without requiring manual specification.",
            "correct_answer": true,
            "explanation": "Prophet automatically places potential changepoints throughout the time series and uses a sparse prior to determine which ones are actually active, requiring no manual intervention."
          },
          {
            "id": 4,
            "type": "coding",
            "question": "Write Python code to create a Prophet model with multiplicative seasonality, US holidays, and a custom 'monthly' seasonality with period 30.5 days.",
            "correct_answer": "from prophet import Prophet\nimport holidays\nus_holidays = holidays.US()\nholiday_df = pd.DataFrame({'holiday': 'US_holiday', 'ds': pd.to_datetime(list(us_holidays.keys()))})\nmodel = Prophet(seasonality_mode='multiplicative', holidays=holiday_df)\nmodel.add_seasonality(name='monthly', period=30.5, fourier_order=5)",
            "explanation": "This creates a Prophet model with multiplicative seasonality, adds US holidays as a DataFrame, and includes custom monthly seasonality using add_seasonality() with the specified period."
          },
          {
            "id": 5,
            "type": "multiple_choice",
            "question": "What does increasing the changepoint_prior_scale parameter in Prophet do?",
            "options": [
              "Increases the number of seasonal components",
              "Makes the trend more flexible by allowing more changepoints",
              "Increases the prediction interval width",
              "Strengthens holiday effects"
            ],
            "correct_answer": 1,
            "explanation": "changepoint_prior_scale controls the flexibility of the trend. Higher values (e.g., 0.5) allow more changepoints to be active, creating a more flexible trend that adapts to changes. Lower values (e.g., 0.01) create smoother, more conservative trends."
          },
          {
            "id": 6,
            "type": "true_false",
            "question": "Prophet uses Fourier series to model seasonality, allowing it to capture complex seasonal patterns with different amplitudes and shapes.",
            "correct_answer": true,
            "explanation": "Prophet models seasonality using Fourier series: s(t) = Σ[aₙcos(2πnt/P) + bₙsin(2πnt/P)]. The number of Fourier terms (fourier_order) controls the complexity of the seasonal pattern that can be captured."
          },
          {
            "id": 7,
            "type": "multiple_choice",
            "question": "When performing cross-validation with Prophet using cross_validation(), what does the 'horizon' parameter specify?",
            "options": [
              "The total length of the training data",
              "How far into the future to make forecasts for evaluation",
              "The number of days between changepoints",
              "The seasonal period length"
            ],
            "correct_answer": 1,
            "explanation": "The 'horizon' parameter specifies the forecast distance for evaluation (e.g., '365 days'). Combined with 'initial' (training size) and 'period' (evaluation frequency), it creates a rolling cross-validation scheme to assess forecast accuracy."
          }
        ]
      }
    }
  ]
}